# src/data/dataset_multiplier.py
"""
Dataset Multiplication Engine
ëŒ€ìš©ëŸ‰ ì¦ê°• ë°ì´í„°ì…‹ ìƒì„±ê¸° - ì²´ê³„ì ì´ê³  íš¨ìœ¨ì ì¸ ë°ì´í„° í™•ì¥
"""

import os
import cv2
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import albumentations as A
from albumentations.pytorch import ToTensorV2
import json
from tqdm import tqdm
from icecream import ic
import fire
from collections import defaultdict
import shutil
from datetime import datetime


class DatasetMultiplier:
    """ì²´ê³„ì ì¸ ë°ì´í„°ì…‹ ì¦ê°• ë° ì €ì¥ ì‹œìŠ¤í…œ"""
    
    def __init__(self, 
                 source_dir: str = "data/raw",
                 output_base_dir: str = "data/augmented_datasets",
                 csv_file: str = "data/raw/metadata/train.csv",
                 meta_file: str = "data/raw/metadata/meta.csv"):
        """
        ì´ˆê¸°í™”
        Args:
            source_dir: ì›ë³¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
            output_base_dir: ì¦ê°• ë°ì´í„°ì…‹ ì €ì¥ ê¸°ë³¸ ë””ë ‰í† ë¦¬
            csv_file: ì›ë³¸ CSV íŒŒì¼
            meta_file: ë©”íƒ€ë°ì´í„° íŒŒì¼
        """
        self.source_dir = Path(source_dir)
        self.output_base_dir = Path(output_base_dir)
        self.csv_file = csv_file
        self.meta_file = meta_file
        
        # ë°ì´í„° ë¡œë“œ
        self.df = pd.read_csv(csv_file)
        self.meta_df = pd.read_csv(meta_file)
        
        # í´ë˜ìŠ¤ ì •ë³´
        self.class_info = dict(zip(self.meta_df['target'], self.meta_df['class_name']))
        self.class_distribution = self.df['target'].value_counts().sort_index()
        
        ic("ğŸ“Š Dataset Multiplier ì´ˆê¸°í™” ì™„ë£Œ")
        ic(f"ì›ë³¸ ìƒ˜í”Œ ìˆ˜: {len(self.df)}")
        ic(f"í´ë˜ìŠ¤ ìˆ˜: {len(self.class_info)}")
        ic(f"í´ë˜ìŠ¤ë³„ ë¶„í¬: {dict(self.class_distribution.head())}")

    def generate_progressive_datasets(self, base_multiplier: int = 10):
        """Progressive trainingìš© 3ë‹¨ê³„ ë°ì´í„°ì…‹ ìƒì„±"""
        
        progressive_configs = [
            ("phase1_mild_20deg", "phase1_mild", base_multiplier),
            ("phase2_variety_60deg", "phase2_variety", base_multiplier),
            ("phase3_full_90deg", "phase3_full", base_multiplier)
        ]
        
        results = {}
        
        for dataset_name, strategy, multiplier in progressive_configs:
            ic(f"\nğŸ¯ {dataset_name} ìƒì„± ì‹œì‘")
            try:
                output_path = self.save_augmented_dataset(dataset_name, strategy, multiplier)
                results[dataset_name] = {
                    'status': 'success',
                    'path': output_path,
                    'strategy': strategy,
                    'multiplier': multiplier
                }
                ic(f"âœ… {dataset_name} ì™„ë£Œ")
            except Exception as e:
                ic(f"âŒ {dataset_name} ì‹¤íŒ¨: {e}")
                results[dataset_name] = {
                    'status': 'failed',
                    'error': str(e)
                }
        
        return results   
        
    def create_augmentation_strategy(self, strategy_name: str, intensity: float = 0.7) -> A.Compose:
        """ì¦ê°• ì „ëµ ìƒì„±"""
        if strategy_name == "phase1_mild":
            # Phase 1: Mild Acclimation (Â±20Â°)
            return A.Compose([
                A.Rotate(
                    limit=20, 
                    border_mode=cv2.BORDER_CONSTANT,
                    fill=(255, 255, 255), 
                    p=0.8
                ),
                A.GaussNoise(
                    std_range=(0.01, 0.05), 
                    p=0.3
                ),
            ])
            
        elif strategy_name == "phase2_variety":
            # Phase 2: Introduce Variety (Â±60Â° + discrete 90Â°)
            return A.Compose([
                A.OneOf([
                    # Continuous rotations
                    A.Rotate(
                        limit=60, 
                        border_mode=cv2.BORDER_CONSTANT, 
                        fill=(255, 255, 255), 
                        p=1.0
                    ),
                    # Discrete 90Â° rotations
                    A.RandomRotate90(p=1.0),
                ], p=0.9),               
                A.Perspective(
                    scale=(0.01, 0.04), 
                    keep_size=True, 
                    p=0.3
                ),
            ])
            
        elif strategy_name == "phase3_full":
            # Phase 3: Full Robustness (Â±90Â°)
            return A.Compose([
                A.OneOf([
                    # Full range continuous rotations
                    A.Rotate(
                        limit=90, 
                        border_mode=cv2.BORDER_CONSTANT, 
                        fill=(255, 255, 255), 
                        p=1.0
                    ),
                    # Discrete 90Â° rotations
                    A.RandomRotate90(p=1.0),
                    # Horizontal/Vertical flips
                    A.HorizontalFlip(p=1.0),
                    A.VerticalFlip(p=1.0),
                ], p=0.95),                
                A.Perspective(
                    scale=(0.01, 0.04), 
                    keep_size=True, 
                    p=0.3
                ),
            ])
               
        elif strategy_name == "volume_focused":
            # V1: ëŒ€ìš©ëŸ‰ ìƒì„± - ë‹¤ì–‘í•œ ë³€í˜•
            return A.Compose([
                A.OneOf([
                    A.Rotate(limit=30, border_mode=cv2.BORDER_CONSTANT, fill=(255, 255, 255), p=1.0), # Added parameter
                    A.Rotate(limit=45, border_mode=cv2.BORDER_CONSTANT, fill=(255, 255, 255), p=1.0), # Added parameter
                    A.Rotate(limit=60, border_mode=cv2.BORDER_CONSTANT, fill=(255, 255, 255), p=1.0), # Added parameter
                    A.Rotate(limit=90, border_mode=cv2.BORDER_CONSTANT, fill=(255, 255, 255), p=1.0), # Added parameter
                ], p=0.8),
                A.Perspective(
                    scale=(0.01, 0.04),
                    keep_size=True,
                    fill=(255, 255, 255),  # Added parameter
                    p=0.4
                ),
            ])
            
        elif strategy_name == "test_focused":
            # V2: í…ŒìŠ¤íŠ¸ ì¡°ê±´ ì‹œë®¬ë ˆì´ì…˜
            return A.Compose([
            # íšŒì „ ì¤‘ì‹¬ (554% ì°¨ì´ í•´ê²°)
            A.Rotate(
            limit=25, 
            border_mode=cv2.BORDER_CONSTANT, 
            p=0.9
            ),
            
            
            # ì›ê·¼ ì™œê³¡
            A.Perspective(
            scale=(0.05, 0.15), 
            keep_size=True, 
            p=0.5
            ),
            ])
        elif strategy_name == "balanced":
            # V3: í´ë˜ìŠ¤ ê· í˜• - ë³´ìˆ˜ì  ì¦ê°•
            return A.Compose([
            A.Rotate(
                limit=20, 
                border_mode=cv2.BORDER_CONSTANT, 
                p=0.7
            ),
            A.Perspective(
                scale=(0.01, 0.04), 
                keep_size=True, 
                p=0.3
            ),
            ])
        else:
            raise ValueError(f"Unknown strategy: {strategy_name}")
    
    def calculate_multiplication_targets(self, strategy: str, target_multiplier: int) -> Dict[int, int]:
        """í´ë˜ìŠ¤ë³„ ì¦ê°• ëª©í‘œ ê³„ì‚°"""
        targets = {}
        
        if strategy == "balanced":
            # ëª¨ë“  í´ë˜ìŠ¤ë¥¼ ìµœëŒ€ í´ë˜ìŠ¤ í¬ê¸°ë¡œ ë§ì¶¤
            max_class_size = self.class_distribution.max()
            target_size = max_class_size * target_multiplier
            
            for class_id, current_size in self.class_distribution.items():
                targets[class_id] = target_size
                
        else:
            # ê· ë“± ì¦ê°•
            for class_id, current_size in self.class_distribution.items():
                targets[class_id] = current_size * target_multiplier
        
        return targets
    
    def generate_augmented_samples(self, 
                                 image_path: str, 
                                 transform: A.Compose, 
                                 num_augmentations: int,
                                 base_filename: str) -> List[Tuple[np.ndarray, str]]:
        """ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ ì—¬ëŸ¬ ì¦ê°• ìƒ˜í”Œ ìƒì„±"""
        
        # ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(image_path)
        if image is None:
            ic(f"âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}")
            return []
        
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        augmented_samples = []
        
        # ì›ë³¸ í¬í•¨
        augmented_samples.append((image, f"{base_filename}_original.jpg"))
        
        # ì¦ê°• ìƒ˜í”Œ ìƒì„±
        for i in range(num_augmentations - 1):  # -1 because we include original
            try:
                augmented = transform(image=image)
                aug_image = augmented['image']
                aug_filename = f"{base_filename}_aug_{i:03d}.jpg"
                augmented_samples.append((aug_image, aug_filename))
            except Exception as e:
                ic(f"âš ï¸ ì¦ê°• ì‹¤íŒ¨: {base_filename}, aug {i}: {e}")
                continue
        
        return augmented_samples
    
    def save_augmented_dataset(self, 
                             dataset_name: str,
                             strategy: str,
                             target_multiplier: int,
                             batch_size: int = 100) -> str:
        """ì¦ê°• ë°ì´í„°ì…‹ ìƒì„± ë° ì €ì¥"""
        
        ic(f"ğŸš€ {dataset_name} ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘")
        ic(f"ì „ëµ: {strategy}, ë°°ìˆ˜: {target_multiplier}x")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        output_dir = self.output_base_dir / dataset_name
        images_dir = output_dir / "train"
        metadata_dir = output_dir / "metadata"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        images_dir.mkdir(parents=True, exist_ok=True)
        metadata_dir.mkdir(parents=True, exist_ok=True)
        
        # ì¦ê°• ì „ëµ ë° ëª©í‘œ ì„¤ì •
        transform = self.create_augmentation_strategy(strategy)
        multiplication_targets = self.calculate_multiplication_targets(strategy, target_multiplier)
        
        # ìƒˆ CSV ë°ì´í„° ì¤€ë¹„
        new_csv_data = []
        generation_stats = defaultdict(int)
        
        # í´ë˜ìŠ¤ë³„ ì²˜ë¦¬
        total_classes = len(self.class_distribution)
        
        for class_idx, (class_id, current_count) in enumerate(self.class_distribution.items()):
            target_count = multiplication_targets[class_id]
            augmentations_per_image = target_count // current_count
            
            ic(f"í´ë˜ìŠ¤ {class_id} ({self.class_info[class_id]}): "
               f"{current_count} â†’ {target_count} ({augmentations_per_image}x)")
            
            # í•´ë‹¹ í´ë˜ìŠ¤ ì´ë¯¸ì§€ë“¤ ê°€ì ¸ì˜¤ê¸°
            class_images = self.df[self.df['target'] == class_id]
            
            # ë°°ì¹˜ ì²˜ë¦¬
            batch_count = 0
            batch_images = []
            
            with tqdm(total=len(class_images), 
                     desc=f"í´ë˜ìŠ¤ {class_id} ì²˜ë¦¬ì¤‘",
                     leave=False) as pbar:
                
                for _, row in class_images.iterrows():
                    source_image_path = self.source_dir / "train" / row['ID']
                    base_filename = Path(row['ID']).stem
                    
                    # ì¦ê°• ìƒ˜í”Œ ìƒì„±
                    augmented_samples = self.generate_augmented_samples(
                        str(source_image_path),
                        transform,
                        augmentations_per_image,
                        f"class_{class_id}_{base_filename}"
                    )
                    
                    # ë°°ì¹˜ì— ì¶”ê°€
                    for aug_image, aug_filename in augmented_samples:
                        batch_images.append((aug_image, aug_filename, class_id))
                        
                        # CSV ë°ì´í„° ì¶”ê°€
                        new_csv_data.append({
                            'ID': aug_filename,
                            'target': class_id
                        })
                    
                    # ë°°ì¹˜ ì €ì¥
                    if len(batch_images) >= batch_size:
                        self._save_batch(batch_images, images_dir)
                        generation_stats[class_id] += len(batch_images)
                        batch_images = []
                        batch_count += 1
                    
                    pbar.update(1)
                
                # ë‚¨ì€ ë°°ì¹˜ ì €ì¥
                if batch_images:
                    self._save_batch(batch_images, images_dir)
                    generation_stats[class_id] += len(batch_images)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        self._save_metadata(new_csv_data, metadata_dir, dataset_name, generation_stats)
        
        # ìƒì„± ìš”ì•½
        total_generated = sum(generation_stats.values())
        ic(f"âœ… {dataset_name} ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ")
        ic(f"ì´ ìƒì„± ìƒ˜í”Œ: {total_generated:,}")
        ic(f"ì €ì¥ ìœ„ì¹˜: {output_dir}")
        
        return str(output_dir)
    
    def _save_batch(self, batch_images: List[Tuple[np.ndarray, str, int]], output_dir: Path):
        """ì´ë¯¸ì§€ ë°°ì¹˜ ì €ì¥"""
        for image, filename, class_id in batch_images:
            output_path = output_dir / filename
            
            # RGB to BGR for cv2
            image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
            cv2.imwrite(str(output_path), image_bgr)
    
    def _save_metadata(self, csv_data: List[Dict], metadata_dir: Path, 
                      dataset_name: str, stats: Dict[int, int]):
        """ë©”íƒ€ë°ì´í„° íŒŒì¼ë“¤ ì €ì¥"""
        
        # ìƒˆ train.csv ì €ì¥
        new_df = pd.DataFrame(csv_data)
        train_csv_path = metadata_dir / "train.csv"
        new_df.to_csv(train_csv_path, index=False)
        
        # ì›ë³¸ meta.csv ë³µì‚¬
        meta_csv_path = metadata_dir / "meta.csv"
        shutil.copy2(self.meta_file, meta_csv_path)
        
        # ìƒì„± í†µê³„ ì €ì¥
        stats_path = metadata_dir / "generation_stats.json"
        generation_info = {
            'dataset_name': dataset_name,
            'generation_timestamp': datetime.now().isoformat(),
            'total_samples': sum(stats.values()),
            'original_samples': len(self.df),
            'multiplication_factor': sum(stats.values()) / len(self.df),
            'class_statistics': {
                str(class_id): {
                    'generated_count': count,
                    'class_name': self.class_info[class_id]
                } for class_id, count in stats.items()
            }
        }
        
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(generation_info, f, indent=2, ensure_ascii=False)
        
        ic(f"ğŸ“‹ ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_dir}")
    
    def generate_all_variants(self):
        """ëª¨ë“  ë°ì´í„°ì…‹ ë³€í˜• ìƒì„±"""
        
        variants = [
            ("v1_volume_20x", "volume_focused", 20),
            ("v2_test_focused_10x", "test_focused", 10),
            ("v3_balanced_15x", "balanced", 15),
            ("v3_double_2x", "double", 2)
        ]
        
        results = {}
        
        for dataset_name, strategy, multiplier in variants:
            ic(f"\nğŸ¯ {dataset_name} ìƒì„± ì‹œì‘")
            try:
                output_path = self.save_augmented_dataset(dataset_name, strategy, multiplier)
                results[dataset_name] = {
                    'status': 'success',
                    'path': output_path,
                    'strategy': strategy,
                    'multiplier': multiplier
                }
                ic(f"âœ… {dataset_name} ì™„ë£Œ")
            except Exception as e:
                ic(f"âŒ {dataset_name} ì‹¤íŒ¨: {e}")
                results[dataset_name] = {
                    'status': 'failed',
                    'error': str(e)
                }
        
        # ì „ì²´ ìš”ì•½ ì €ì¥
        summary_path = self.output_base_dir / "generation_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        ic(f"ğŸ‰ ëª¨ë“  ë³€í˜• ìƒì„± ì™„ë£Œ. ìš”ì•½: {summary_path}")
        return results
    def generate_stratified_kfold_datasets(self, k: int = 5, multiplier: int = 10, 
                                        strategy: str = "phase1_mild"):
        """
        Stratified K-fold ë°ì´í„°ì…‹ ìƒì„± - ì†ŒìŠ¤ ë ˆë²¨ì—ì„œ ë¶„í•  í›„ ì¦ê°•
        
        Args:
            k: í´ë“œ ìˆ˜
            multiplier: ì¦ê°• ë°°ìˆ˜
            strategy: ì¦ê°• ì „ëµ
        """
        ic(f"ğŸ¯ Stratified {k}-fold ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘")
        ic(f"ì „ëµ: {strategy}, ë°°ìˆ˜: {multiplier}x")
        
        from sklearn.model_selection import StratifiedKFold
        
        # í´ë˜ìŠ¤ë³„ë¡œ ì†ŒìŠ¤ ì´ë¯¸ì§€ ê·¸ë£¹í™”
        source_groups = defaultdict(list)
        for _, row in self.df.iterrows():
            source_groups[row['target']].append(row['ID'])
        
        # ê° í´ë˜ìŠ¤ì—ì„œ ì†ŒìŠ¤ ì´ë¯¸ì§€ ê¸°ì¤€ìœ¼ë¡œ K-fold ë¶„í• 
        kfold_splits = []
        
        for fold_idx in range(k):
            train_sources = []
            val_sources = []
            
            for class_id, sources in source_groups.items():
                # í´ë˜ìŠ¤ ë‚´ì—ì„œ K-fold ë¶„í• 
                fold_size = len(sources) // k
                start_idx = fold_idx * fold_size
                end_idx = start_idx + fold_size if fold_idx < k-1 else len(sources)
                
                # í˜„ì¬ í´ë“œë¥¼ validationìœ¼ë¡œ
                val_sources.extend(sources[start_idx:end_idx])
                # ë‚˜ë¨¸ì§€ë¥¼ trainingìœ¼ë¡œ
                train_sources.extend(sources[:start_idx] + sources[end_idx:])
            
            kfold_splits.append({
                'fold': fold_idx,
                'train_sources': train_sources,
                'val_sources': val_sources
            })
        
        ic(f"âœ… {k}ê°œ í´ë“œ ë¶„í•  ì™„ë£Œ")
        
        # ê° í´ë“œë³„ ë°ì´í„°ì…‹ ìƒì„±
        results = {}
        
        for split_info in kfold_splits:
            fold_idx = split_info['fold']
            dataset_name = f"{strategy}_fold_{fold_idx}"
            
            ic(f"\nğŸ“ Fold {fold_idx} ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
            
            try:
                fold_path = self._generate_fold_dataset(
                    split_info, 
                    dataset_name, 
                    strategy, 
                    multiplier
                )
                
                results[dataset_name] = {
                    'status': 'success',
                    'path': fold_path,
                    'fold': fold_idx,
                    'train_sources': len(split_info['train_sources']),
                    'val_sources': len(split_info['val_sources'])
                }
                ic(f"âœ… Fold {fold_idx} ì™„ë£Œ: {fold_path}")
                
            except Exception as e:
                ic(f"âŒ Fold {fold_idx} ì‹¤íŒ¨: {e}")
                results[dataset_name] = {
                    'status': 'failed',
                    'error': str(e),
                    'fold': fold_idx
                }
        
        return results

    def _generate_fold_dataset(self, split_info: Dict, dataset_name: str, 
                            strategy: str, multiplier: int) -> str:
        """ë‹¨ì¼ í´ë“œ ë°ì´í„°ì…‹ ìƒì„±"""
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        output_dir = self.output_base_dir / dataset_name
        train_dir = output_dir / "train"
        val_dir = output_dir / "val"
        metadata_dir = output_dir / "metadata"
        
        for dir_path in [train_dir, val_dir, metadata_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # ì¦ê°• ì „ëµ ìƒì„±
        transform = self.create_augmentation_strategy(strategy)
        
        # Train/Val CSV ë°ì´í„° ì¤€ë¹„
        train_csv_data = []
        val_csv_data = []
        
        # Training set ìƒì„± (ì¦ê°• ì ìš©)
        ic(f"ğŸ“Š Training set ìƒì„± ì¤‘ ({len(split_info['train_sources'])} ì†ŒìŠ¤)")
        for source_filename in tqdm(split_info['train_sources'], desc="Training ì¦ê°•"):
            # ì›ë³¸ ë°ì´í„°ì—ì„œ í•´ë‹¹ ì†ŒìŠ¤ ì°¾ê¸°
            source_row = self.df[self.df['ID'] == source_filename].iloc[0]
            source_path = self.source_dir / "train" / source_filename
            
            if not source_path.exists():
                ic(f"âš ï¸ ì†ŒìŠ¤ íŒŒì¼ ì—†ìŒ: {source_path}")
                continue
            
            # ì¦ê°• ìƒ˜í”Œ ìƒì„±
            base_filename = Path(source_filename).stem
            class_id = source_row['target']
            
            augmented_samples = self.generate_augmented_samples(
                str(source_path),
                transform,
                multiplier,
                f"fold_train_class_{class_id}_{base_filename}"
            )
            
            # ì¦ê°•ëœ ìƒ˜í”Œë“¤ ì €ì¥
            for aug_image, aug_filename in augmented_samples:
                # ì´ë¯¸ì§€ ì €ì¥
                output_path = train_dir / aug_filename
                image_bgr = cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR)
                cv2.imwrite(str(output_path), image_bgr)
                
                # CSV ë°ì´í„° ì¶”ê°€
                train_csv_data.append({
                    'ID': aug_filename,
                    'target': class_id
                })
        
        # Validation set ìƒì„± (ì›ë³¸ë§Œ, ì¦ê°• ì—†ìŒ)
        ic(f"ğŸ“Š Validation set ìƒì„± ì¤‘ ({len(split_info['val_sources'])} ì†ŒìŠ¤)")
        for source_filename in tqdm(split_info['val_sources'], desc="Validation ë³µì‚¬"):
            source_row = self.df[self.df['ID'] == source_filename].iloc[0]
            source_path = self.source_dir / "train" / source_filename
            
            if not source_path.exists():
                continue
            
            # ì›ë³¸ ì´ë¯¸ì§€ ë³µì‚¬ (ì¦ê°• ì—†ìŒ)
            val_filename = f"val_original_{source_filename}"
            val_path = val_dir / val_filename
            shutil.copy2(source_path, val_path)
            
            # CSV ë°ì´í„° ì¶”ê°€
            val_csv_data.append({
                'ID': val_filename,
                'target': source_row['target']
            })
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        self._save_fold_metadata(train_csv_data, val_csv_data, metadata_dir, dataset_name)
        
        ic(f"âœ… Fold ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {output_dir}")
        ic(f"   Training samples: {len(train_csv_data)}")
        ic(f"   Validation samples: {len(val_csv_data)}")
        
        return str(output_dir)

    def _save_fold_metadata(self, train_data: List[Dict], val_data: List[Dict], 
                        metadata_dir: Path, dataset_name: str):
        """K-fold ë©”íƒ€ë°ì´í„° ì €ì¥"""
        
        # Train/Val CSV ë¶„ë¦¬ ì €ì¥
        train_df = pd.DataFrame(train_data)
        val_df = pd.DataFrame(val_data)
        
        train_df.to_csv(metadata_dir / "train.csv", index=False)
        val_df.to_csv(metadata_dir / "val.csv", index=False)
        
        # ì›ë³¸ meta.csv ë³µì‚¬
        shutil.copy2(self.meta_file, metadata_dir / "meta.csv")
        
        # í´ë“œ ì •ë³´ ì €ì¥
        fold_info = {
            'dataset_name': dataset_name,
            'generation_timestamp': datetime.now().isoformat(),
            'train_samples': len(train_data),
            'val_samples': len(val_data),
            'data_leakage': 'prevented',
            'split_method': 'source_level_stratified'
        }
        
        with open(metadata_dir / "fold_info.json", 'w', encoding='utf-8') as f:
            json.dump(fold_info, f, indent=2, ensure_ascii=False)

def main():
    """Fire CLI ì¸í„°í˜ì´ìŠ¤"""
    fire.Fire(DatasetMultiplier)


if __name__ == "__main__":
    main()