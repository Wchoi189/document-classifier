"""
src/utils/visual_verification.py

ì‹œê°ì  ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ - ì¦ê°•ëœ í›ˆë ¨ ì´ë¯¸ì§€ì™€ ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì¡°ê±´ ë¹„êµ
Visual verification script - Compare augmented training images with real test conditions
"""

import os
import cv2
import numpy as np

from pathlib import Path
import pandas as pd
from typing import List, Dict, Tuple, Optional
import random
from PIL import Image
import fire

from src.data.augmentation import get_configurable_transforms, get_train_transforms, get_valid_transforms
from src.utils.config_utils import load_config
import matplotlib.pyplot as plt
import matplotlib as mpl

# Set the font family to NanumGothic
mpl.rcParams['font.family'] = 'NanumGothic'
mpl.rcParams['axes.unicode_minus'] = False



class VisualVerificationTool:
    """í›ˆë ¨ ë°ì´í„° ì¦ê°•ê³¼ í…ŒìŠ¤íŠ¸ ì¡°ê±´ ë¹„êµë¥¼ ìœ„í•œ ì‹œê°ì  ê²€ì¦ ë„êµ¬"""
    
    def __init__(self, config_path: str = "configs/config.yaml"):
        """
        ì´ˆê¸°í™”
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.config = load_config(config_path)
        self.setup_paths()
        self.setup_transforms()
        
    def setup_paths(self):
        """ê²½ë¡œ ì„¤ì •"""
        self.train_dir = Path(self.config['data']['root_dir']) / 'train'
        self.test_dir = Path(self.config['data']['root_dir']) / 'test'
        self.output_dir = Path('outputs/visual_verification')
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"âœ… í›ˆë ¨ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬: {self.train_dir}")
        print(f"âœ… í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬: {self.test_dir}")
        print(f"âœ… ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
    
    def setup_transforms(self):
        """ë‹¤ì–‘í•œ ì¦ê°• ì „ëµ ì„¤ì •"""
        img_size = self.config['data']['image_size']
        mean = self.config['data']['mean']
        std = self.config['data']['std']
        
        # ê¸°ë³¸ ì¦ê°• (ê¸°ì¡´)
        self.transform_basic = get_train_transforms(img_size, img_size, mean, std)
        
        # ë¬¸ì„œ íŠ¹í™” ì¦ê°• (ì•½í•¨)
        self.transform_document = get_configurable_transforms(
            img_size, img_size, mean, std, 
            {'strategy': 'document', 'intensity': 0.5}
        )
        
        # ê°•ë ¥í•œ ì¦ê°• (í…ŒìŠ¤íŠ¸ ì¡°ê±´ ì‹œë®¬ë ˆì´ì…˜)
        self.transform_robust = get_configurable_transforms(
            img_size, img_size, mean, std,
            {'strategy': 'robust', 'intensity': 0.8}
        )
        
        # ê²€ì¦ìš© (ì¦ê°• ì—†ìŒ)
        self.transform_valid = get_valid_transforms(img_size, img_size, mean, std)
        
        print("âœ… ëª¨ë“  ì¦ê°• ì „ëµ ì„¤ì • ì™„ë£Œ")
    
    def load_sample_images(self, n_samples: int = 5) -> Tuple[List[np.ndarray], List[str]]:
        """í›ˆë ¨ ì´ë¯¸ì§€ ìƒ˜í”Œ ë¡œë“œ"""
        train_files = list(self.train_dir.glob('*.jpg'))
        if len(train_files) == 0:
            train_files = list(self.train_dir.glob('*.png'))
        
        if len(train_files) == 0:
            raise FileNotFoundError(f"í›ˆë ¨ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.train_dir}")
        
        # ëœë¤ ìƒ˜í”Œë§
        selected_files = random.sample(train_files, min(n_samples, len(train_files)))
        
        images = []
        filenames = []
        
        for file_path in selected_files:
            # OpenCVë¡œ ì´ë¯¸ì§€ ë¡œë“œ (RGB ë³€í™˜)
            img = cv2.imread(str(file_path))
            if img is not None:
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                images.append(img)
                filenames.append(file_path.name)
                print(f"âœ… ë¡œë“œë¨: {file_path.name} - í¬ê¸°: {img.shape}")
            else:
                print(f"âš ï¸ ë¡œë“œ ì‹¤íŒ¨: {file_path.name}")
        
        return images, filenames
    
    def load_test_samples(self, n_samples: int = 5) -> Tuple[List[np.ndarray], List[str]]:
        """í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒ˜í”Œ ë¡œë“œ"""
        test_files = list(self.test_dir.glob('*.jpg'))
        if len(test_files) == 0:
            test_files = list(self.test_dir.glob('*.png'))
        
        if len(test_files) == 0:
            print(f"âš ï¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.test_dir}")
            return [], []
        
        # ëœë¤ ìƒ˜í”Œë§
        selected_files = random.sample(test_files, min(n_samples, len(test_files)))
        
        images = []
        filenames = []
        
        for file_path in selected_files:
            img = cv2.imread(str(file_path))
            if img is not None:
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                images.append(img)
                filenames.append(file_path.name)
                print(f"âœ… í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¡œë“œë¨: {file_path.name}")
        
        return images, filenames
    
    def apply_augmentation(self, image: np.ndarray, transform) -> np.ndarray:
        """ì¦ê°• ì ìš© ë° ì‹œê°í™”ë¥¼ ìœ„í•œ ì—­ì •ê·œí™”"""
        try:
            # Albumentations ì ìš©
            augmented = transform(image=image)
            tensor_img = augmented['image']
            
            # í…ì„œë¥¼ numpyë¡œ ë³€í™˜ (C, H, W) -> (H, W, C)
            if hasattr(tensor_img, 'numpy'):
                img_np = tensor_img.numpy().transpose(1, 2, 0)
            else:
                img_np = tensor_img.permute(1, 2, 0).numpy()
            
            # ì—­ì •ê·œí™” (ì •ê·œí™”ëœ ì´ë¯¸ì§€ë¥¼ ì›ë˜ ë²”ìœ„ë¡œ)
            mean = np.array(self.config['data']['mean'])
            std = np.array(self.config['data']['std'])
            img_np = img_np * std + mean
            img_np = np.clip(img_np, 0, 1)
            
            return img_np
            
        except Exception as e:
            print(f"âŒ ì¦ê°• ì ìš© ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ì‹œ ì›ë³¸ ì´ë¯¸ì§€ ë°˜í™˜ (0-1 ë²”ìœ„ë¡œ ì •ê·œí™”)
            return image.astype(np.float32) / 255.0
    
    def create_comparison_grid(self, 
                             original_images: List[np.ndarray], 
                             filenames: List[str],
                             test_images: Optional[List[np.ndarray]] = None,
                             test_filenames: Optional[List[str]] = None) -> plt.Figure:
        """ë¹„êµ ê·¸ë¦¬ë“œ ìƒì„±"""
        
        n_train = len(original_images)
        n_test = len(test_images) if test_images else 0
        
        # ê·¸ë¦¬ë“œ í¬ê¸° ê³„ì‚°: í›ˆë ¨ ì´ë¯¸ì§€ë‹¹ 4ê°œ ì¦ê°• + í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë“¤
        cols = 5  # ì›ë³¸ + 3ê°œ ì¦ê°• + 1ê°œ ì—¬ë°±
        rows = n_train + (1 if n_test > 0 else 0)  # í›ˆë ¨ ì´ë¯¸ì§€ í–‰ë“¤ + í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ í–‰
        
        fig, axes = plt.subplots(rows, cols, figsize=(20, 4 * rows))
        if rows == 1:
            axes = axes.reshape(1, -1)
        
        # í›ˆë ¨ ì´ë¯¸ì§€ë“¤ê³¼ ì¦ê°• ë¹„êµ
        for i, (img, filename) in enumerate(zip(original_images, filenames)):
            # ì›ë³¸ ì´ë¯¸ì§€
            axes[i, 0].imshow(img)
            axes[i, 0].set_title(f'ì›ë³¸\n{filename}', fontsize=10)
            axes[i, 0].axis('off')
            
            # ê¸°ë³¸ ì¦ê°•
            aug_basic = self.apply_augmentation(img, self.transform_basic)
            axes[i, 1].imshow(aug_basic)
            axes[i, 1].set_title('ê¸°ë³¸ ì¦ê°•', fontsize=10)
            axes[i, 1].axis('off')
            
            # ë¬¸ì„œ ì¦ê°•
            aug_document = self.apply_augmentation(img, self.transform_document)
            axes[i, 2].imshow(aug_document)
            axes[i, 2].set_title('ë¬¸ì„œ ì¦ê°•', fontsize=10)
            axes[i, 2].axis('off')
            
            # ê°•ë ¥í•œ ì¦ê°•
            aug_robust = self.apply_augmentation(img, self.transform_robust)
            axes[i, 3].imshow(aug_robust)
            axes[i, 3].set_title('ê°•ë ¥í•œ ì¦ê°•', fontsize=10)
            axes[i, 3].axis('off')
            
            # ë¹ˆ ê³µê°„
            axes[i, 4].axis('off')
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë“¤ í‘œì‹œ (ë§ˆì§€ë§‰ í–‰)
        if n_test > 0:
            test_row = n_train
            for j in range(cols):
                if j < len(test_images):
                    axes[test_row, j].imshow(test_images[j])
                    axes[test_row, j].set_title(f'í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ\n{test_filenames[j]}', fontsize=10)
                    axes[test_row, j].axis('off')
                else:
                    axes[test_row, j].axis('off')
        
        plt.suptitle('í›ˆë ¨ ë°ì´í„° ì¦ê°• vs ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì¡°ê±´ ë¹„êµ', fontsize=16, y=0.98)
        plt.tight_layout()
        return fig
    
    def analyze_augmentation_coverage(self, 
                                    original_images: List[np.ndarray], 
                                    n_variations: int = 10) -> Dict:
        """ì¦ê°• ì»¤ë²„ë¦¬ì§€ ë¶„ì„ - ë‹¤ì–‘í•œ ì¦ê°• ê²°ê³¼ ìƒì„±"""
        print(f"ğŸ” ì¦ê°• ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ì¤‘... (ë³€í˜• {n_variations}ê°œ)")
        
        analysis_results = {
            'brightness_range': [],
            'blur_levels': [],
            'rotation_angles': [],
            'perspective_scores': []
        }
        
        for img in original_images:
            brightness_values = []
            blur_scores = []
            
            # ì—¬ëŸ¬ ë²ˆ ì¦ê°• ì ìš©í•˜ì—¬ ë‹¤ì–‘ì„± ì¸¡ì •
            for _ in range(n_variations):
                aug_img = self.apply_augmentation(img, self.transform_robust)
                
                # ë°ê¸° ì¸¡ì •
                brightness = np.mean(aug_img)
                brightness_values.append(brightness)
                
                # ë¸”ëŸ¬ ì •ë„ ì¸¡ì • (ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚°)
                gray = cv2.cvtColor((aug_img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)
                blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()
                blur_scores.append(blur_score)
            
            analysis_results['brightness_range'].append((min(brightness_values), max(brightness_values)))
            analysis_results['blur_levels'].append((min(blur_scores), max(blur_scores)))
        
        return analysis_results
    
    def generate_comprehensive_report(self, n_train_samples: int = 5, n_test_samples: int = 5):
        """ì¢…í•© ì‹œê°ì  ê²€ì¦ ë³´ê³ ì„œ ìƒì„±"""
        print("ğŸš€ ì¢…í•© ì‹œê°ì  ê²€ì¦ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        # 1. í›ˆë ¨ ì´ë¯¸ì§€ ë¡œë“œ
        print("\nğŸ“¥ í›ˆë ¨ ì´ë¯¸ì§€ ë¡œë“œ ì¤‘...")
        train_images, train_filenames = self.load_sample_images(n_train_samples)
        
        # 2. í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¡œë“œ
        print("\nğŸ“¥ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¡œë“œ ì¤‘...")
        test_images, test_filenames = self.load_test_samples(n_test_samples)
        
        # 3. ë¹„êµ ê·¸ë¦¬ë“œ ìƒì„±
        print("\nğŸ“Š ë¹„êµ ê·¸ë¦¬ë“œ ìƒì„± ì¤‘...")
        comparison_fig = self.create_comparison_grid(
            train_images, train_filenames, 
            test_images, test_filenames
        )
        
        # 4. ê²°ê³¼ ì €ì¥
        output_path = self.output_dir / 'augmentation_vs_test_comparison.png'
        comparison_fig.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close(comparison_fig)
        
        # 5. ì¦ê°• ì»¤ë²„ë¦¬ì§€ ë¶„ì„
        print("\nğŸ” ì¦ê°• ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ì¤‘...")
        coverage_analysis = self.analyze_augmentation_coverage(train_images)
        
        # 6. ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„±
        self.create_analysis_summary(coverage_analysis, len(train_images), len(test_images))
        
        print(f"\nâœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ!")
        print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_path}")
        print(f"ğŸ“ ìš”ì•½ íŒŒì¼: {self.output_dir / 'analysis_summary.txt'}")
        
        return str(output_path)
    
    def create_analysis_summary(self, coverage_analysis: Dict, n_train: int, n_test: int):
        """ë¶„ì„ ìš”ì•½ í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±"""
        summary_path = self.output_dir / 'analysis_summary.txt'
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("# ì‹œê°ì  ê²€ì¦ ë¶„ì„ ìš”ì•½\n")
            f.write("=" * 50 + "\n\n")
            
            f.write(f"## ê¸°ë³¸ ì •ë³´\n")
            f.write(f"- ë¶„ì„ëœ í›ˆë ¨ ì´ë¯¸ì§€: {n_train}ê°œ\n")
            f.write(f"- ë¶„ì„ëœ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {n_test}ê°œ\n")
            f.write(f"- ì¦ê°• ì „ëµ: basic, document, robust\n\n")
            
            f.write(f"## ì¦ê°• ì»¤ë²„ë¦¬ì§€ ë¶„ì„\n")
            if coverage_analysis['brightness_range']:
                brightness_ranges = coverage_analysis['brightness_range']
                min_brightness = min([r[0] for r in brightness_ranges])
                max_brightness = max([r[1] for r in brightness_ranges])
                f.write(f"- ë°ê¸° ë³€í™” ë²”ìœ„: {min_brightness:.3f} ~ {max_brightness:.3f}\n")
            
            if coverage_analysis['blur_levels']:
                blur_ranges = coverage_analysis['blur_levels']
                min_blur = min([r[0] for r in blur_ranges])
                max_blur = max([r[1] for r in blur_ranges])
                f.write(f"- ë¸”ëŸ¬ ë ˆë²¨ ë²”ìœ„: {min_blur:.1f} ~ {max_blur:.1f}\n")
            
            f.write(f"\n## ê¶Œì¥ì‚¬í•­\n")
            f.write(f"1. ìƒì„±ëœ ë¹„êµ ì´ë¯¸ì§€ë¥¼ í™•ì¸í•˜ì—¬ í…ŒìŠ¤íŠ¸ ì¡°ê±´ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€í† \n")
            f.write(f"2. ì¦ê°• ê°•ë„ê°€ ë¶€ì¡±í•˜ë©´ config.yamlì—ì„œ intensity ê°’ ì¦ê°€\n")
            f.write(f"3. ì¦ê°•ì´ ê³¼ë„í•˜ë©´ intensity ê°’ ê°ì†Œ\n")
            f.write(f"4. íŠ¹ì • ì¦ê°• ê¸°ë²•ì´ íš¨ê³¼ì ì´ë©´ í•´ë‹¹ ì „ëµ ì‚¬ìš©\n")


def main():
    """ë©”ì¸ í•¨ìˆ˜ - Fire CLI ì¸í„°í˜ì´ìŠ¤"""
    fire.Fire(VisualVerificationTool)


if __name__ == "__main__":
    main()