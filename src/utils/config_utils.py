"""
Configuration utilities to handle both legacy YAML configs and new Hydra configs
"""
import yaml
from omegaconf import DictConfig, OmegaConf
from pathlib import Path
from typing import Dict, Any, Union
from icecream import ic


def load_config(config_path: Union[str, Path]) -> Dict[str, Any]:
    """
    Enhanced config loading with Hydra defaults support
    """
    if isinstance(config_path, (DictConfig, dict)):
        # ì´ë¯¸ ë¡œë“œëœ configì¸ ê²½ìš°
        if isinstance(config_path, DictConfig):
            result = OmegaConf.to_container(config_path, resolve=True)
            if not isinstance(result, dict):
                raise TypeError("Config must be a dictionary after conversion")
            return {str(k): v for k, v in result.items()}
        return config_path
    
    # íŒŒì¼ì—ì„œ ë¡œë“œí•˜ëŠ” ê²½ìš°
    config_path = Path(config_path)
    
    # Hydra defaultsê°€ ìˆëŠ” ê²½ìš° ìˆ˜ë™ ë³‘í•©
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            temp_config = yaml.safe_load(f)
        
        if 'defaults' in temp_config:
            ic("ğŸ”§ Hydra defaults ê°ì§€, ìˆ˜ë™ ë³‘í•© ì‹¤í–‰")
            config = load_and_merge_hydra_defaults(config_path)
        else:
            ic("ğŸ“„ ì¼ë°˜ YAML ë¡œë“œ")
            config = temp_config
            
    except Exception as e:
        ic(f"âŒ Config ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise
    
    return config


def ensure_required_config_sections(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    í•„ìˆ˜ config ì„¹ì…˜ë“¤ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ ì¶”ê°€
    """
    ic("ğŸ”§ í•„ìˆ˜ config ì„¹ì…˜ í™•ì¸ ë° ì¶”ê°€")
    
    # ê¸°ë³¸ model ì„¤ì •
    if 'model' not in config:
        ic("â• model ì„¹ì…˜ ì¶”ê°€")
        config['model'] = {
            'name': 'resnet50',
            'pretrained': True,
            'dropout_rate': 0.5
        }
    
    # ê¸°ë³¸ optimizer ì„¤ì •
    if 'optimizer' not in config:
        ic("â• optimizer ì„¹ì…˜ ì¶”ê°€")
        config['optimizer'] = {
            'name': 'AdamW',
            'learning_rate': 0.001,
            'weight_decay': 0.01
        }
    
    # ê¸°ë³¸ scheduler ì„¤ì •
    if 'scheduler' not in config:
        ic("â• scheduler ì„¹ì…˜ ì¶”ê°€")
        config['scheduler'] = {
            'name': 'CosineAnnealingWarmRestarts',
            'T_0': 10,
            'T_mult': 2,
            'eta_min': 0.00001
        }
    
    # train ì„¹ì…˜ ë³´ì™„
    if 'train' in config:
        train_config = config['train']
        if 'early_stopping' not in train_config:
            ic("â• early_stopping ì„¹ì…˜ ì¶”ê°€")
            train_config['early_stopping'] = {
                'patience': 8,
                'metric': 'val_f1',
                'mode': 'max'
            }
    
    # paths ì„¹ì…˜ ë³´ì™„
    if 'paths' not in config:
        ic("â• paths ì„¹ì…˜ ì¶”ê°€")
        config['paths'] = {
            'output_dir': 'outputs',
            'prediction_dir': 'predictions',
            'model_dir': 'models',
            'batch_dir': 'batch',
            'batch_summary_filename': 'batch_summary.csv'
        }
    
    # logging ì„¹ì…˜ ë³´ì™„
    if 'logging' not in config:
        ic("â• logging ì„¹ì…˜ ì¶”ê°€")
        config['logging'] = {
            'checkpoint_dir': str(Path(config['paths']['output_dir']) / config['paths']['model_dir']),
            'log_dir': str(Path(config['paths']['output_dir']) / 'logs')
        }
    
    ic(f"ë³´ì™„ ì™„ë£Œ, ìµœì¢… í‚¤ë“¤: {list(config.keys())}")
    return config


def normalize_config_structure(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Enhanced config normalization with required sections
    """
    # ê¸°ì¡´ normalize ë¡œì§
    normalized = config.copy()
    
    # Handle different optimizer config formats
    if 'optimizer' in normalized:
        opt_config = normalized['optimizer']
        if 'learning_rate' in opt_config and 'lr' not in opt_config:
            opt_config['lr'] = opt_config['learning_rate']
        elif 'lr' in opt_config and 'learning_rate' not in opt_config:
            opt_config['learning_rate'] = opt_config['lr']
    
    # í•„ìˆ˜ ì„¹ì…˜ ë³´ì¥
    normalized = ensure_required_config_sections(normalized)
    
    return normalized


# ê¸°ì¡´ í•¨ìˆ˜ë“¤ ìœ ì§€ (backward compatibility)
def safe_config_get(config: Dict[str, Any], key: str, default: Any = None) -> Any:
    """Safely get a value from config dictionary."""
    if not isinstance(config, dict):
        raise TypeError(f"Config must be a dictionary, got {type(config)}")
    return config.get(key, default)


def convert_config_types(config: Dict[str, Any]) -> Dict[str, Any]:
    """Convert string config values to appropriate types."""
    # Convert seed to int
    if 'seed' in config and isinstance(config['seed'], str):
        config['seed'] = int(config['seed'])
    
    # Convert data config
    if 'data' in config and isinstance(config['data'], dict):
        data_config = config['data']
        
        # Convert numeric values
        if 'image_size' in data_config and isinstance(data_config['image_size'], str):
            data_config['image_size'] = int(data_config['image_size'])
        
        if 'val_size' in data_config and isinstance(data_config['val_size'], str):
            data_config['val_size'] = float(data_config['val_size'])
            
        if 'num_workers' in data_config and isinstance(data_config['num_workers'], str):
            data_config['num_workers'] = int(data_config['num_workers'])
        
        # Convert lists (mean, std)
        if 'mean' in data_config and isinstance(data_config['mean'], str):
            data_config['mean'] = [float(x.strip()) for x in data_config['mean'].split(',')]
        elif 'mean' in data_config and isinstance(data_config['mean'], list):
            data_config['mean'] = [float(x) for x in data_config['mean']]
            
        if 'std' in data_config and isinstance(data_config['std'], str):
            data_config['std'] = [float(x.strip()) for x in data_config['std'].split(',')]
        elif 'std' in data_config and isinstance(data_config['std'], list):
            data_config['std'] = [float(x) for x in data_config['std']]
    
    return config

# Backward compatibility function
def load_config_legacy(config_path: str) -> Dict[str, Any]:
    """Legacy config loading function for backward compatibility."""
    return load_config(config_path)

# def normalize_config_structure(config: Dict[str, Any]) -> Dict[str, Any]:
#     """
#     Normalize config structure to ensure compatibility between old and new formats.
    
#     Args:
#         config: Configuration dictionary
        
#     Returns:
#         Normalized configuration dictionary
#     """
#     normalized = config.copy()
    
#     # Handle different optimizer config formats
#     if 'optimizer' in normalized:
#         opt_config = normalized['optimizer']
#         if 'learning_rate' in opt_config and 'lr' not in opt_config:
#             opt_config['lr'] = opt_config['learning_rate']
#         elif 'lr' in opt_config and 'learning_rate' not in opt_config:
#             opt_config['learning_rate'] = opt_config['lr']
    
#     # Handle scheduler config normalization
#     if 'scheduler' in normalized:
#         sched_config = normalized['scheduler']
#         if 'name' in sched_config:
#             # Convert scheduler name to params structure if needed
#             if sched_config['name'] == 'CosineAnnealingWarmRestarts':
#                 if 'scheduler_params' not in normalized.get('train', {}):
#                     if 'train' not in normalized:
#                         normalized['train'] = {}
#                     normalized['train']['scheduler_params'] = {
#                         'T_0': sched_config.get('T_0', 10),
#                         'T_mult': sched_config.get('T_mult', 2),
#                         'eta_min': sched_config.get('eta_min', 0.0)
#                     }
#                     normalized['train']['scheduler'] = sched_config['name']
    
#     # Ensure paths structure exists
#     if 'paths' not in normalized:
#         normalized['paths'] = {
#             'output_dir': 'outputs',
#             'prediction_dir': 'predictions', 
#             'model_dir': 'models',
#             'batch_dir': 'batch',
#             'batch_summary_filename': 'batch_summary.csv'
#         }
    
#     # Ensure logging structure exists for early stopping
#     if 'logging' not in normalized:
#         normalized['logging'] = {
#             'checkpoint_dir': str(Path(normalized['paths']['output_dir']) / normalized['paths']['model_dir']),
#             'log_dir': str(Path(normalized['paths']['output_dir']) / 'logs')
#         }
    
#     return normalized


def get_experiment_name(config: Dict[str, Any]) -> str:
    """
    Get experiment name from config, handling both old and new formats.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        Experiment name string
    """
    # Try new Hydra format first
    if 'experiment' in config and isinstance(config['experiment'], dict):
        return config['experiment'].get('name', 'default_experiment')
    
    # Try WandB run name
    if 'wandb' in config and 'name' in config['wandb']:
        return config['wandb']['name']
    
    # Default fallback
    return 'default_experiment'


def update_wandb_config(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Update WandB configuration with experiment info from Hydra config.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        Updated configuration dictionary
    """
    config = config.copy()
    
    if 'experiment' in config and 'wandb' in config:
        experiment = config['experiment']
        wandb_config = config['wandb']
        
        # Update WandB name if not set
        if 'name' not in wandb_config or not wandb_config['name']:
            wandb_config['name'] = experiment.get('name', 'hydra_experiment')
        
        # Update tags
        if 'tags' in experiment:
            existing_tags = wandb_config.get('tags', [])
            experiment_tags = experiment['tags']
            # Merge tags, avoiding duplicates
            all_tags = list(set(existing_tags + experiment_tags))
            wandb_config['tags'] = all_tags
        
        # Update notes
        if 'description' in experiment:
            wandb_config['notes'] = experiment['description']
    
    return config


def print_config_summary(config: Dict[str, Any]) -> None:
    """
    Print a summary of the current configuration.
    
    Args:
        config: Configuration dictionary
    """
    print("\n" + "="*50)
    print("ğŸ“‹ CONFIGURATION SUMMARY")
    print("="*50)
    
    # Experiment info
    exp_name = get_experiment_name(config)
    print(f"ğŸ·ï¸  Experiment: {exp_name}")
    
    if 'experiment' in config:
        exp = config['experiment']
        if 'description' in exp:
            print(f"ğŸ“ Description: {exp['description']}")
        if 'tags' in exp:
            print(f"ğŸ·ï¸  Tags: {', '.join(exp['tags'])}")
    
    # Model info
    if 'model' in config:
        model = config['model']
        print(f"ğŸ§  Model: {model.get('name', 'unknown')}")
        print(f"ğŸ“ Pretrained: {model.get('pretrained', False)}")
    
    # Training info
    if 'train' in config:
        train = config['train']
        print(f"ğŸ‹ï¸  Epochs: {train.get('epochs', 'unknown')}")
        print(f"ğŸ“¦ Batch Size: {train.get('batch_size', 'unknown')}")
    
    # Optimizer info
    if 'optimizer' in config:
        opt = config['optimizer']
        lr = opt.get('learning_rate', opt.get('lr', 'unknown'))
        print(f"âš™ï¸  Optimizer: {opt.get('name', 'unknown')} (lr={lr})")
    
    # Scheduler info
    if 'scheduler' in config:
        sched = config['scheduler']
        print(f"ğŸ“ˆ Scheduler: {sched.get('name', 'unknown')}")
    
    # Data info
    if 'data' in config:
        data = config['data']
        print(f"ğŸ–¼ï¸  Image Size: {data.get('image_size', 'unknown')}")
        print(f"ğŸ”„ Augmentation: {data.get('use_document_augmentation', False)}")
    
    print("="*50 + "\n")




#Reusable configuration utilities to handle Hydra/OmegaConf safely
def safe_get(config, key_path, default=None):
    """
    Safely get nested config values without Pylance warnings
    
    Args:
        config: Configuration dict (might be None)
        key_path: String like 'data.augmentation.strategy' or list ['data', 'augmentation', 'strategy']
        default: Default value if not found
    
    Returns:
        Config value or default
    """
    if config is None or not isinstance(config, dict):
        return default
    
    # Convert string path to list
    if isinstance(key_path, str):
        keys = key_path.split('.')
    else:
        keys = key_path
    
    current = config
    for key in keys:
        if not isinstance(current, dict) or key not in current:
            return default
        current = current[key]
    
    return current

# def safe_config_get(config: Dict[str, Any], key: str, default: Any = None) -> Any:
#     """Safely get a value from config dictionary."""
#     if not isinstance(config, dict):
#         raise TypeError(f"Config must be a dictionary, got {type(config)}")
#     return config.get(key, default)


# def convert_config_types(config: Dict[str, Any]) -> Dict[str, Any]:
#     """Convert string config values to appropriate types."""
#     # Convert seed to int
#     if 'seed' in config and isinstance(config['seed'], str):
#         config['seed'] = int(config['seed'])
    
#     # Convert data config
#     if 'data' in config and isinstance(config['data'], dict):
#         data_config = config['data']
        
#         # Convert numeric values
#         if 'image_size' in data_config and isinstance(data_config['image_size'], str):
#             data_config['image_size'] = int(data_config['image_size'])
        
#         if 'val_size' in data_config and isinstance(data_config['val_size'], str):
#             data_config['val_size'] = float(data_config['val_size'])
            
#         if 'num_workers' in data_config and isinstance(data_config['num_workers'], str):
#             data_config['num_workers'] = int(data_config['num_workers'])
        
#         # Convert lists (mean, std)
#         if 'mean' in data_config and isinstance(data_config['mean'], str):
#             data_config['mean'] = [float(x.strip()) for x in data_config['mean'].split(',')]
#         elif 'mean' in data_config and isinstance(data_config['mean'], list):
#             data_config['mean'] = [float(x) for x in data_config['mean']]
            
#         if 'std' in data_config and isinstance(data_config['std'], str):
#             data_config['std'] = [float(x.strip()) for x in data_config['std'].split(',')]
#         elif 'std' in data_config and isinstance(data_config['std'], list):
#             data_config['std'] = [float(x) for x in data_config['std']]
    
#     # Convert train config
#     if 'train' in config and isinstance(config['train'], dict):
#         train_config = config['train']
#         if 'batch_size' in train_config and isinstance(train_config['batch_size'], str):
#             train_config['batch_size'] = int(train_config['batch_size'])
    
#     # Convert model config
#     if 'model' in config and isinstance(config['model'], dict):
#         model_config = config['model']
#         if 'pretrained' in model_config and isinstance(model_config['pretrained'], str):
#             model_config['pretrained'] = model_config['pretrained'].lower() in ('true', '1', 'yes')
    
#     # Convert optimizer config
#     if 'optimizer' in config and isinstance(config['optimizer'], dict):
#         opt_config = config['optimizer']
#         if 'learning_rate' in opt_config and isinstance(opt_config['learning_rate'], str):
#             opt_config['learning_rate'] = float(opt_config['learning_rate'])
#         if 'weight_decay' in opt_config and isinstance(opt_config['weight_decay'], str):
#             opt_config['weight_decay'] = float(opt_config['weight_decay'])
#         if 'momentum' in opt_config and isinstance(opt_config['momentum'], str):
#             opt_config['momentum'] = float(opt_config['momentum'])
    
#     # Convert scheduler config
#     if 'scheduler' in config and isinstance(config['scheduler'], dict):
#         sched_config = config['scheduler']
#         for key in ['T_0', 'T_mult', 'T_max', 'step_size']:
#             if key in sched_config and isinstance(sched_config[key], str):
#                 sched_config[key] = int(sched_config[key])
#         for key in ['eta_min', 'gamma']:
#             if key in sched_config and isinstance(sched_config[key], str):
#                 sched_config[key] = float(sched_config[key])
    
#     return config

def safe_dict_get(dictionary: Dict[Union[str, int], Any], key: str) -> Any:
    """
    Safely get a value from a dictionary that might have string or int keys.
    
    This handles the case where classification_report returns a dict with
    mixed key types (str for class names, int for numeric labels).
    
    Args:
        dictionary: Dictionary with potentially mixed key types
        key: String key to look up
        
    Returns:
        Value if found, None otherwise
    """
    # First try direct string key access
    if key in dictionary:
        return dictionary[key]
    
    # If not found, try converting key to int and back
    try:
        int_key = int(key)
        if int_key in dictionary:
            return dictionary[int_key]
    except (ValueError, TypeError):
        pass
    
    return None


def safe_classification_report_access(report: Dict[Any, Any], class_key: str) -> Dict[str, Any]:
    """
    Safely access classification report metrics for a specific class.
    
    Args:
        report: Classification report dictionary from sklearn
        class_key: Class name/label as string
        
    Returns:
        Class metrics dictionary or empty dict if not found
    """
    # Use .get() method which is type-safe
    class_metrics = report.get(class_key)
    
    if class_metrics is not None and isinstance(class_metrics, dict):
        return class_metrics
    
    # Try with integer conversion if string key doesn't work
    try:
        int_key = int(class_key)
        class_metrics = report.get(int_key)
        if class_metrics is not None and isinstance(class_metrics, dict):
            return class_metrics
    except (ValueError, TypeError):
        pass
    
    return {}


def get_classification_metrics(report: Dict[Any, Any], class_name: Union[str, int]) -> Dict[str, float]:
    """
    Type-safe getter for classification report metrics with default values.
    
    Args:
        report: Classification report from sklearn
        class_name: Class name or label
        
    Returns:
        Dictionary with precision, recall, f1-score, support
    """
    key = str(class_name)
    metrics = report.get(key, {})
    
    if isinstance(metrics, dict):
        return {
            'precision': float(metrics.get('precision', 0.0)),
            'recall': float(metrics.get('recall', 0.0)),
            'f1-score': float(metrics.get('f1-score', 0.0)),
            'support': int(metrics.get('support', 0))
        }
    
    return {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}


def load_and_merge_hydra_defaults(config_path: Union[str, Path]) -> Dict[str, Any]:
    """
    Hydra defaultsë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë¡œë“œí•˜ê³  ë³‘í•©í•˜ëŠ” í•¨ìˆ˜
    Conservative testerì—ì„œ Hydra context ì—†ì´ ì‚¬ìš©í•  ë•Œ í•„ìš”
    """
    config_path = Path(config_path)
    ic(f"ğŸ”§ Hydra defaults ìˆ˜ë™ ë³‘í•© ì‹œì‘: {config_path}")
    
    # 1. ë©”ì¸ config ë¡œë“œ
    with open(config_path, 'r', encoding='utf-8') as f:
        main_config = yaml.safe_load(f)
    
    ic("ë©”ì¸ config ë¡œë“œ ì™„ë£Œ")
    
    # 2. defaults ì„¹ì…˜ í™•ì¸
    defaults = main_config.get('defaults', [])
    if not defaults:
        ic("âš ï¸ defaults ì„¹ì…˜ ì—†ìŒ, ë©”ì¸ configë§Œ ë°˜í™˜")
        return main_config
    
    ic(f"ë°œê²¬ëœ defaults: {defaults}")
    
    # 3. ë³‘í•©í•  config ìˆ˜ì§‘
    merged_config = {}
    configs_dir = config_path.parent
    
    for default_item in defaults:
        if default_item == '_self_':
            # _self_ëŠ” ë‚˜ì¤‘ì— ì²˜ë¦¬
            continue
        elif isinstance(default_item, dict):
            # {category: name} í˜•íƒœ
            for category, name in default_item.items():
                config_file = configs_dir / category / f"{name}.yaml"
                if config_file.exists():
                    ic(f"âœ… {category}/{name}.yaml ë¡œë“œ ì¤‘")
                    with open(config_file, 'r', encoding='utf-8') as f:
                        category_config = yaml.safe_load(f)
                    merged_config[category] = category_config
                else:
                    ic(f"âš ï¸ {config_file} íŒŒì¼ ì—†ìŒ")
        elif isinstance(default_item, str):
            # ë‹¨ì¼ íŒŒì¼ëª…
            config_file = configs_dir / f"{default_item}.yaml"
            if config_file.exists():
                ic(f"âœ… {default_item}.yaml ë¡œë“œ ì¤‘")
                with open(config_file, 'r', encoding='utf-8') as f:
                    item_config = yaml.safe_load(f)
                merged_config.update(item_config)
    
    # 4. ë©”ì¸ configì™€ ë³‘í•© (_self_ ì²˜ë¦¬)
    for key, value in main_config.items():
        if key != 'defaults':  # defaultsëŠ” ì œì™¸
            merged_config[key] = value
    
    ic(f"ë³‘í•© ì™„ë£Œ, ìµœì¢… í‚¤ë“¤: {list(merged_config.keys())}")
    return merged_config

def ensure_required_config_sections(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    í•„ìˆ˜ config ì„¹ì…˜ë“¤ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ ì¶”ê°€
    """
    ic("ğŸ”§ í•„ìˆ˜ config ì„¹ì…˜ í™•ì¸ ë° ì¶”ê°€")
    
    # ğŸ”§ FIXED: Add missing global settings
    if 'seed' not in config:
        ic("â• seed ì¶”ê°€")
        config['seed'] = 42
    
    if 'device' not in config:
        ic("â• device ì¶”ê°€")
        config['device'] = 'cuda'
    
    # ğŸ”§ FIXED: Ensure experiment section exists
    if 'experiment' not in config:
        ic("â• experiment ì„¹ì…˜ ì¶”ê°€")
        config['experiment'] = {
            'name': 'default_experiment',
            'description': 'Default experiment configuration',
            'tags': ['default']
        }
    
    # ê¸°ë³¸ model ì„¤ì •
    if 'model' not in config:
        ic("â• model ì„¹ì…˜ ì¶”ê°€")
        config['model'] = {
            'name': 'resnet50',
            'pretrained': True,
            'dropout_rate': 0.5
        }
    
    # ê¸°ë³¸ optimizer ì„¤ì •
    if 'optimizer' not in config:
        ic("â• optimizer ì„¹ì…˜ ì¶”ê°€")
        config['optimizer'] = {
            'name': 'AdamW',
            'learning_rate': 0.001,
            'weight_decay': 0.01
        }
    
    # ê¸°ë³¸ scheduler ì„¤ì •
    if 'scheduler' not in config:
        ic("â• scheduler ì„¹ì…˜ ì¶”ê°€")
        config['scheduler'] = {
            'name': 'CosineAnnealingWarmRestarts',
            'T_0': 10,
            'T_mult': 2,
            'eta_min': 0.00001
        }
    
    # ğŸ”§ FIXED: Ensure augmentation section exists
    if 'augmentation' not in config:
        ic("â• augmentation ì„¹ì…˜ ì¶”ê°€")
        config['augmentation'] = {
            'enabled': True,
            'strategy': 'basic',
            'intensity': 0.3
        }
    
    # train ì„¹ì…˜ ë³´ì™„
    if 'train' in config:
        train_config = config['train']
        if 'early_stopping' not in train_config:
            ic("â• early_stopping ì„¹ì…˜ ì¶”ê°€")
            train_config['early_stopping'] = {
                'patience': 8,
                'metric': 'val_f1',
                'mode': 'max'
            }
    
    # paths ì„¹ì…˜ ë³´ì™„
    if 'paths' not in config:
        ic("â• paths ì„¹ì…˜ ì¶”ê°€")
        config['paths'] = {
            'output_dir': 'outputs',
            'prediction_dir': 'predictions',
            'model_dir': 'models',
            'batch_dir': 'batch',
            'batch_summary_filename': 'batch_summary.csv'
        }
    
    # logging ì„¹ì…˜ ë³´ì™„
    if 'logging' not in config:
        ic("â• logging ì„¹ì…˜ ì¶”ê°€")
        config['logging'] = {
            'checkpoint_dir': str(Path(config['paths']['output_dir']) / config['paths']['model_dir']),
            'log_dir': str(Path(config['paths']['output_dir']) / 'logs')
        }
    
    ic(f"ë³´ì™„ ì™„ë£Œ, ìµœì¢… í‚¤ë“¤: {list(config.keys())}")
    return config