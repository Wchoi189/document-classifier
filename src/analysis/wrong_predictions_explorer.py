"""
src/analysis/wrong_predictions_explorer.py

ì˜ëª»ëœ ì˜ˆì¸¡ íƒìƒ‰ê¸° - ì˜¤ë¶„ë¥˜ëœ ìƒ˜í”Œë“¤ì˜ ì‹œê°ì  ë¶„ì„ ë„êµ¬
Wrong predictions explorer - Visual analysis tool for misclassified samples
"""

import os
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import json
from collections import defaultdict, Counter
import fire

from src.utils.config_utils import load_config, get_classification_metrics
from sklearn.metrics import classification_report, confusion_matrix


class WrongPredictionsExplorer:
    """ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ë¶„ì„ì„ ìœ„í•œ íƒìƒ‰ ë„êµ¬"""
    
    def __init__(self, config_path: str = "configs/config.yaml"):
        """
        ì´ˆê¸°í™”
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.config = load_config(config_path)
        self.setup_paths()
        self.load_class_info()
        
    def setup_paths(self):
        """ê²½ë¡œ ì„¤ì •"""
        self.data_dir = Path(self.config['data']['root_dir'])
        self.output_dir = Path('outputs/wrong_predictions_analysis')
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"âœ… ë°ì´í„° ë””ë ‰í† ë¦¬: {self.data_dir}")
        print(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥: {self.output_dir}")
    
    def load_class_info(self):
        """í´ë˜ìŠ¤ ì •ë³´ ë¡œë“œ"""
        meta_file = self.config['data']['meta_file']
        if os.path.exists(meta_file):
            self.meta_df = pd.read_csv(meta_file)
            self.class_names = dict(zip(self.meta_df['target'], self.meta_df['class_name']))
            print(f"âœ… í´ë˜ìŠ¤ ì •ë³´ ë¡œë“œ: {len(self.class_names)}ê°œ í´ë˜ìŠ¤")
        else:
            print(f"âš ï¸ ë©”íƒ€ íŒŒì¼ ì—†ìŒ, ê¸°ë³¸ í´ë˜ìŠ¤ëª… ì‚¬ìš©: {meta_file}")
            self.class_names = {i: f"class_{i}" for i in range(17)}
    
    def load_predictions(self, predictions_csv: str, ground_truth_csv: Optional[str] = None) -> pd.DataFrame:
        """ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ ë¡œë“œ"""
        print(f"ğŸ“¥ ì˜ˆì¸¡ ê²°ê³¼ ë¡œë“œ: {predictions_csv}")
        
        # ì˜ˆì¸¡ ê²°ê³¼ ë¡œë“œ
        df_pred = pd.read_csv(predictions_csv)
        required_cols = ['filename', 'predicted_target', 'confidence']
        
        if not all(col in df_pred.columns for col in required_cols):
            raise ValueError(f"ì˜ˆì¸¡ CSVì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {required_cols}")
        
        # ì •ë‹µ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
        if ground_truth_csv and os.path.exists(ground_truth_csv):
            print(f"ğŸ“¥ ì •ë‹µ ë°ì´í„° ë¡œë“œ: {ground_truth_csv}")
            df_true = pd.read_csv(ground_truth_csv)
            
            # íŒŒì¼ëª…ìœ¼ë¡œ ì¡°ì¸ (í™•ì¥ì ì œê±°)
            df_pred['join_key'] = df_pred['filename'].str.replace(r'\.(jpg|jpeg|png)$', '', regex=True)
            df_true['join_key'] = df_true['ID'].str.replace(r'\.(jpg|jpeg|png)$', '', regex=True)
            
            df_merged = pd.merge(df_pred, df_true, on='join_key', how='inner')
            
            if df_merged.empty:
                raise ValueError("ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ ë°ì´í„° ê°„ ë§¤ì¹­ë˜ëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            print(f"âœ… ë§¤ì¹­ëœ ìƒ˜í”Œ: {len(df_merged)}ê°œ")
            return df_merged
        else:
            print("âš ï¸ ì •ë‹µ ë°ì´í„° ì—†ìŒ - ì˜ˆì¸¡ ê²°ê³¼ë§Œ ë¶„ì„")
            df_pred['target'] = -1  # ë”ë¯¸ ì •ë‹µ
            return df_pred
    
    def identify_wrong_predictions(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
        """ì˜ëª»ëœ ì˜ˆì¸¡ ì‹ë³„ ë° ë¶„ì„"""
        if 'target' not in df.columns:
            print("âš ï¸ ì •ë‹µ ë¼ë²¨ì´ ì—†ì–´ ì˜¤ë¶„ë¥˜ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return df, {}
        
        # ì˜¤ë¶„ë¥˜ ì‹ë³„
        df['is_correct'] = df['predicted_target'] == df['target']
        df['error_type'] = df.apply(
            lambda row: f"{self.class_names.get(row['target'], row['target'])} â†’ {self.class_names.get(row['predicted_target'], row['predicted_target'])}" 
            if not row['is_correct'] else 'Correct', axis=1
        )
        
        wrong_preds = df[~df['is_correct']].copy()
        
        # ì˜¤ë¶„ë¥˜ í†µê³„
        error_stats = {
            'total_samples': len(df),
            'correct_predictions': df['is_correct'].sum(),
            'wrong_predictions': len(wrong_preds),
            'accuracy': df['is_correct'].mean(),
            'error_rate': 1 - df['is_correct'].mean()
        }
        
        print(f"ğŸ“Š ì˜¤ë¶„ë¥˜ ë¶„ì„ ê²°ê³¼:")
        print(f"   ì „ì²´ ìƒ˜í”Œ: {error_stats['total_samples']}")
        print(f"   ì •í™•í•œ ì˜ˆì¸¡: {error_stats['correct_predictions']}")
        print(f"   ì˜ëª»ëœ ì˜ˆì¸¡: {error_stats['wrong_predictions']}")
        print(f"   ì •í™•ë„: {error_stats['accuracy']:.3f}")
        
        return wrong_preds, error_stats
    
    def analyze_error_patterns(self, wrong_preds: pd.DataFrame) -> Dict:
        """ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„"""
        if wrong_preds.empty:
            return {}
        
        print("ğŸ” ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„ ì¤‘...")
        
        patterns = {}
        
        # 1. í´ë˜ìŠ¤ë³„ ì˜¤ë¶„ë¥˜ ë¹ˆë„
        patterns['class_errors'] = wrong_preds.groupby('target').size().to_dict()
        
        # 2. ê°€ì¥ ìì£¼ í˜¼ë™ë˜ëŠ” í´ë˜ìŠ¤ ìŒ
        error_pairs = wrong_preds.groupby(['target', 'predicted_target']).size().reset_index(name='count')
        patterns['confusion_pairs'] = error_pairs.nlargest(10, 'count').to_dict('records')
        
        # 3. ì‹ ë¢°ë„ë³„ ì˜¤ë¶„ë¥˜ ë¶„í¬
        confidence_bins = pd.cut(wrong_preds['confidence'], bins=[0, 0.5, 0.7, 0.9, 1.0], 
                               labels=['Very Low (0-0.5)', 'Low (0.5-0.7)', 'Medium (0.7-0.9)', 'High (0.9-1.0)'])
        patterns['confidence_distribution'] = confidence_bins.value_counts().to_dict()
        
        # 4. ë‚®ì€ ì‹ ë¢°ë„ ì˜ˆì¸¡ (ì„ê³„ê°’ ì´í•˜)
        low_confidence_threshold = 0.7
        patterns['low_confidence_errors'] = len(wrong_preds[wrong_preds['confidence'] < low_confidence_threshold])
        patterns['high_confidence_errors'] = len(wrong_preds[wrong_preds['confidence'] >= low_confidence_threshold])
        
        return patterns
    
    def create_error_visualization(self, wrong_preds: pd.DataFrame, patterns: Dict) -> plt.Figure:
        """ì˜¤ë¥˜ ì‹œê°í™” ìƒì„±"""
        fig = plt.figure(figsize=(20, 15))
        
        # 1. í´ë˜ìŠ¤ë³„ ì˜¤ë¶„ë¥˜ ìˆ˜ (ì„œë¸Œí”Œë¡¯ 1)
        ax1 = plt.subplot(2, 3, 1)
        if patterns.get('class_errors'):
            class_error_series = pd.Series(patterns['class_errors'])
            class_error_series.index = [self.class_names.get(idx, f"Class_{idx}") for idx in class_error_series.index]
            class_error_series.plot(kind='bar', ax=ax1)
            ax1.set_title('í´ë˜ìŠ¤ë³„ ì˜¤ë¶„ë¥˜ ê°œìˆ˜', fontsize=12)
            ax1.tick_params(axis='x', rotation=45)
        
        # 2. ì‹ ë¢°ë„ ë¶„í¬ (ì„œë¸Œí”Œë¡¯ 2)
        ax2 = plt.subplot(2, 3, 2)
        if not wrong_preds.empty:
            wrong_preds['confidence'].hist(bins=20, ax=ax2, alpha=0.7, color='red', label='Wrong')
            ax2.set_title('ì˜¤ë¶„ë¥˜ ì‹ ë¢°ë„ ë¶„í¬', fontsize=12)
            ax2.set_xlabel('Confidence Score')
            ax2.set_ylabel('Frequency')
        
        # 3. í˜¼ë™ ë§¤íŠ¸ë¦­ìŠ¤ (ì„œë¸Œí”Œë¡¯ 3-4, í° ì˜ì—­)
        ax3 = plt.subplot(2, 2, 2)
        if 'target' in wrong_preds.columns and not wrong_preds.empty:
            # ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ í˜¼ë™ ë§¤íŠ¸ë¦­ìŠ¤ í•„ìš” - ì—¬ê¸°ì„œëŠ” ì˜¤ë¶„ë¥˜ë§Œ í‘œì‹œ
            error_matrix = wrong_preds.groupby(['target', 'predicted_target']).size().unstack(fill_value=0)
            sns.heatmap(error_matrix, annot=True, fmt='d', cmap='Reds', ax=ax3)
            ax3.set_title('ì˜¤ë¶„ë¥˜ í˜¼ë™ ë§¤íŠ¸ë¦­ìŠ¤', fontsize=12)
        
        # 4. ê°€ì¥ í˜¼ë™ë˜ëŠ” í´ë˜ìŠ¤ ìŒ (ì„œë¸Œí”Œë¡¯ 5)
        ax4 = plt.subplot(2, 3, 5)
        if patterns.get('confusion_pairs'):
            pairs_df = pd.DataFrame(patterns['confusion_pairs'][:5])  # ìƒìœ„ 5ê°œ
            pairs_df['pair_label'] = pairs_df.apply(
                lambda row: f"{self.class_names.get(row['target'], row['target'])} â†’ {self.class_names.get(row['predicted_target'], row['predicted_target'])}", 
                axis=1
            )
            pairs_df.plot(x='pair_label', y='count', kind='bar', ax=ax4)
            ax4.set_title('ê°€ì¥ ìì£¼ í˜¼ë™ë˜ëŠ” í´ë˜ìŠ¤ ìŒ', fontsize=12)
            ax4.tick_params(axis='x', rotation=45)
        
        # 5. ì‹ ë¢°ë„ êµ¬ê°„ë³„ ì˜¤ë¶„ë¥˜ (ì„œë¸Œí”Œë¡¯ 6)
        ax5 = plt.subplot(2, 3, 6)
        if patterns.get('confidence_distribution'):
            conf_dist = pd.Series(patterns['confidence_distribution'])
            conf_dist.plot(kind='bar', ax=ax5, color='orange')
            ax5.set_title('ì‹ ë¢°ë„ êµ¬ê°„ë³„ ì˜¤ë¶„ë¥˜ ë¶„í¬', fontsize=12)
            ax5.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        return fig
    
    def create_sample_gallery(self, wrong_preds: pd.DataFrame, n_samples: int = 20) -> plt.Figure:
        """ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ê°¤ëŸ¬ë¦¬ ìƒì„±"""
        if wrong_preds.empty:
            print("âš ï¸ í‘œì‹œí•  ì˜¤ë¶„ë¥˜ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ë‹¤ì–‘í•œ ì˜¤ë¥˜ ìœ í˜•ì—ì„œ ìƒ˜í”Œ ì„ íƒ
        samples_to_show = []
        
        # 1. ë†’ì€ ì‹ ë¢°ë„ ì˜¤ë¶„ë¥˜ (ëª¨ë¸ì´ í™•ì‹ í–ˆì§€ë§Œ í‹€ë¦° ê²½ìš°)
        high_conf_wrong = wrong_preds[wrong_preds['confidence'] > 0.8].head(5)
        samples_to_show.extend(high_conf_wrong.to_dict('records'))
        
        # 2. ë‚®ì€ ì‹ ë¢°ë„ ì˜¤ë¶„ë¥˜ (ëª¨ë¸ì´ ì• ë§¤í•´í–ˆë˜ ê²½ìš°)
        low_conf_wrong = wrong_preds[wrong_preds['confidence'] < 0.6].head(5)
        samples_to_show.extend(low_conf_wrong.to_dict('records'))
        
        # 3. ë‚˜ë¨¸ì§€ ëœë¤ ìƒ˜í”Œ
        remaining_samples = wrong_preds[~wrong_preds.index.isin(
            list(high_conf_wrong.index) + list(low_conf_wrong.index)
        )].sample(min(n_samples - len(samples_to_show), len(wrong_preds) - len(samples_to_show)), random_state=42)
        samples_to_show.extend(remaining_samples.to_dict('records'))
        
        # ì‹¤ì œ ì´ë¯¸ì§€ ë¡œë“œ ë° ê°¤ëŸ¬ë¦¬ ìƒì„±
        n_cols = 4
        n_rows = (len(samples_to_show) + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4 * n_rows))
        if n_rows == 1:
            axes = axes.reshape(1, -1)
        
        for idx, sample in enumerate(samples_to_show):
            row, col = idx // n_cols, idx % n_cols
            
            # ì´ë¯¸ì§€ ë¡œë“œ
            img_path = self.data_dir / 'train' / sample['filename']
            if not img_path.exists():
                img_path = self.data_dir / 'test' / sample['filename']
            
            if img_path.exists():
                img = cv2.imread(str(img_path))
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                
                axes[row, col].imshow(img)
                
                # íƒ€ì´í‹€ ì •ë³´
                true_class = self.class_names.get(sample.get('target', -1), 'Unknown')
                pred_class = self.class_names.get(sample['predicted_target'], 'Unknown')
                confidence = sample['confidence']
                
                title = f"ì‹¤ì œ: {true_class}\nì˜ˆì¸¡: {pred_class}\nì‹ ë¢°ë„: {confidence:.3f}"
                axes[row, col].set_title(title, fontsize=10)
            else:
                axes[row, col].text(0.5, 0.5, f"ì´ë¯¸ì§€ ì—†ìŒ\n{sample['filename']}", 
                                  ha='center', va='center', transform=axes[row, col].transAxes)
            
            axes[row, col].axis('off')
        
        # ë¹ˆ subplot ì œê±°
        for idx in range(len(samples_to_show), n_rows * n_cols):
            row, col = idx // n_cols, idx % n_cols
            axes[row, col].axis('off')
        
        plt.suptitle(f'ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ê°¤ëŸ¬ë¦¬ (ì´ {len(samples_to_show)}ê°œ)', fontsize=16)
        plt.tight_layout()
        return fig
    
    def create_confidence_analysis(self, df: pd.DataFrame) -> plt.Figure:
        """ì‹ ë¢°ë„ ë¶„ì„ ì°¨íŠ¸ ìƒì„±"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        if 'is_correct' in df.columns:
            correct_preds = df[df['is_correct']]
            wrong_preds = df[~df['is_correct']]
            
            # 1. ì‹ ë¢°ë„ ë¶„í¬ ë¹„êµ
            axes[0, 0].hist(correct_preds['confidence'], bins=30, alpha=0.7, label='ì •ë‹µ', color='green')
            axes[0, 0].hist(wrong_preds['confidence'], bins=30, alpha=0.7, label='ì˜¤ë‹µ', color='red')
            axes[0, 0].set_title('ì •ë‹µ vs ì˜¤ë‹µ ì‹ ë¢°ë„ ë¶„í¬')
            axes[0, 0].set_xlabel('Confidence Score')
            axes[0, 0].legend()
            
            # 2. ì‹ ë¢°ë„ êµ¬ê°„ë³„ ì •í™•ë„
            bins = np.arange(0, 1.1, 0.1)
            df['conf_bin'] = pd.cut(df['confidence'], bins=bins)
            accuracy_by_conf = df.groupby('conf_bin')['is_correct'].agg(['mean', 'count']).reset_index()
            
            axes[0, 1].bar(range(len(accuracy_by_conf)), accuracy_by_conf['mean'], 
                          alpha=0.7, color='blue')
            axes[0, 1].set_title('ì‹ ë¢°ë„ êµ¬ê°„ë³„ ì •í™•ë„')
            axes[0, 1].set_xlabel('Confidence Bins')
            axes[0, 1].set_ylabel('Accuracy')
            axes[0, 1].set_xticks(range(len(accuracy_by_conf)))
            axes[0, 1].set_xticklabels([f"{bin.left:.1f}-{bin.right:.1f}" for bin in accuracy_by_conf['conf_bin']], 
                                     rotation=45)
        
        # 3. í´ë˜ìŠ¤ë³„ í‰ê·  ì‹ ë¢°ë„
        class_confidence = df.groupby('predicted_target')['confidence'].mean().sort_values(ascending=False)
        class_confidence.index = [self.class_names.get(idx, f"Class_{idx}") for idx in class_confidence.index]
        
        axes[1, 0].bar(range(len(class_confidence)), class_confidence.values, color='orange')
        axes[1, 0].set_title('í´ë˜ìŠ¤ë³„ í‰ê·  ì˜ˆì¸¡ ì‹ ë¢°ë„')
        axes[1, 0].set_xlabel('Classes')
        axes[1, 0].set_ylabel('Average Confidence')
        axes[1, 0].set_xticks(range(len(class_confidence)))
        axes[1, 0].set_xticklabels(class_confidence.index, rotation=45, ha='right')
        
        # 4. ì‹ ë¢°ë„ vs ì •í™•ë„ ì‚°ì ë„ (í´ë˜ìŠ¤ë³„)
        if 'is_correct' in df.columns:
            class_stats = df.groupby('predicted_target').agg({
                'confidence': 'mean',
                'is_correct': 'mean'
            }).reset_index()
            
            axes[1, 1].scatter(class_stats['confidence'], class_stats['is_correct'], 
                             s=100, alpha=0.7, color='purple')
            axes[1, 1].set_title('í´ë˜ìŠ¤ë³„ ì‹ ë¢°ë„ vs ì •í™•ë„')
            axes[1, 1].set_xlabel('Average Confidence')
            axes[1, 1].set_ylabel('Accuracy')
            
            # í´ë˜ìŠ¤ ë¼ë²¨ ì¶”ê°€
            for idx, row in class_stats.iterrows():
                class_name = self.class_names.get(row['predicted_target'], f"C{row['predicted_target']}")
                axes[1, 1].annotate(class_name, (row['confidence'], row['is_correct']), 
                                  xytext=(5, 5), textcoords='offset points', fontsize=8)
        
        plt.tight_layout()
        return fig
    
    def generate_html_report(self, wrong_preds: pd.DataFrame, patterns: Dict, error_stats: Dict) -> str:
        """HTML í˜•íƒœì˜ ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>ì˜¤ë¶„ë¥˜ ë¶„ì„ ë³´ê³ ì„œ</title>
            <meta charset="UTF-8">
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
                .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
                .stats {{ display: flex; justify-content: space-around; }}
                .stat-box {{ text-align: center; padding: 10px; background-color: #f9f9f9; border-radius: 5px; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
                .error-high {{ background-color: #ffcccc; }}
                .error-medium {{ background-color: #fff2cc; }}
                .error-low {{ background-color: #ccffcc; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>ğŸ” ì˜¤ë¶„ë¥˜ ë¶„ì„ ë³´ê³ ì„œ</h1>
                <p>ìƒì„± ì‹œê°„: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            </div>
            
            <div class="section">
                <h2>ğŸ“Š ì „ì²´ í†µê³„</h2>
                <div class="stats">
                    <div class="stat-box">
                        <h3>{error_stats.get('total_samples', 0)}</h3>
                        <p>ì „ì²´ ìƒ˜í”Œ</p>
                    </div>
                    <div class="stat-box">
                        <h3>{error_stats.get('correct_predictions', 0)}</h3>
                        <p>ì •ë‹µ ì˜ˆì¸¡</p>
                    </div>
                    <div class="stat-box">
                        <h3>{error_stats.get('wrong_predictions', 0)}</h3>
                        <p>ì˜¤ë‹µ ì˜ˆì¸¡</p>
                    </div>
                    <div class="stat-box">
                        <h3>{error_stats.get('accuracy', 0):.3f}</h3>
                        <p>ì •í™•ë„</p>
                    </div>
                </div>
            </div>
            
            <div class="section">
                <h2>ğŸ¯ ì£¼ìš” ì˜¤ë¥˜ íŒ¨í„´</h2>
        """
        
        # í˜¼ë™ë˜ëŠ” í´ë˜ìŠ¤ ìŒ í‘œ ì¶”ê°€
        if patterns.get('confusion_pairs'):
            html_content += """
                <h3>ê°€ì¥ ìì£¼ í˜¼ë™ë˜ëŠ” í´ë˜ìŠ¤ ìŒ</h3>
                <table>
                    <tr><th>ì‹¤ì œ í´ë˜ìŠ¤</th><th>ì˜ˆì¸¡ í´ë˜ìŠ¤</th><th>ì˜¤ë¥˜ íšŸìˆ˜</th></tr>
            """
            for pair in patterns['confusion_pairs'][:10]:
                true_class = self.class_names.get(pair['target'], f"Class_{pair['target']}")
                pred_class = self.class_names.get(pair['predicted_target'], f"Class_{pair['predicted_target']}")
                html_content += f"""
                    <tr>
                        <td>{true_class}</td>
                        <td>{pred_class}</td>
                        <td>{pair['count']}</td>
                    </tr>
                """
            html_content += "</table>"
        
        # ì‹ ë¢°ë„ ë¶„ì„
        if patterns.get('confidence_distribution'):
            html_content += """
                <h3>ì‹ ë¢°ë„ë³„ ì˜¤ë¶„ë¥˜ ë¶„í¬</h3>
                <table>
                    <tr><th>ì‹ ë¢°ë„ êµ¬ê°„</th><th>ì˜¤ë¶„ë¥˜ ê°œìˆ˜</th></tr>
            """
            for conf_range, count in patterns['confidence_distribution'].items():
                html_content += f"<tr><td>{conf_range}</td><td>{count}</td></tr>"
            html_content += "</table>"
        
        html_content += """
            </div>
            
            <div class="section">
                <h2>ğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­</h2>
                <ul>
                    <li>ì‹ ë¢°ë„ê°€ ë†’ì€ë° í‹€ë¦° ì˜ˆì¸¡ë“¤ì„ ì¤‘ì ì ìœ¼ë¡œ ë¶„ì„</li>
                    <li>ìì£¼ í˜¼ë™ë˜ëŠ” í´ë˜ìŠ¤ ìŒì— ëŒ€í•œ ì¶”ê°€ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ê³ ë ¤</li>
                    <li>ë‚®ì€ ì‹ ë¢°ë„ ì˜ˆì¸¡ì— ëŒ€í•œ ì„ê³„ê°’ ì¡°ì • ê²€í† </li>
                    <li>ì˜¤ë¶„ë¥˜ê°€ ë§ì€ í´ë˜ìŠ¤ì— ëŒ€í•œ ì¶”ê°€ í›ˆë ¨ ë°ì´í„° ìˆ˜ì§‘</li>
                </ul>
            </div>
        </body>
        </html>
        """
        
        # HTML íŒŒì¼ ì €ì¥
        html_path = self.output_dir / 'detailed_analysis_report.html'
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return str(html_path)
    
    def run_comprehensive_analysis(self, 
                                  predictions_csv: str, 
                                  ground_truth_csv: Optional[str] = None,
                                  n_sample_images: int = 20):
        """ì¢…í•© ì˜¤ë¶„ë¥˜ ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ ì¢…í•© ì˜¤ë¶„ë¥˜ ë¶„ì„ ì‹œì‘...")
        
        # 1. ë°ì´í„° ë¡œë“œ
        df = self.load_predictions(predictions_csv, ground_truth_csv)
        
        # 2. ì˜¤ë¶„ë¥˜ ì‹ë³„
        wrong_preds, error_stats = self.identify_wrong_predictions(df)
        
        if wrong_preds.empty:
            print("âœ… ëª¨ë“  ì˜ˆì¸¡ì´ ì •í™•í•©ë‹ˆë‹¤!")
            return
        
        # 3. ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„
        patterns = self.analyze_error_patterns(wrong_preds)
        
        # 4. ì‹œê°í™” ìƒì„±
        print("ğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # ì˜¤ë¥˜ ë¶„ì„ ì°¨íŠ¸
        error_viz = self.create_error_visualization(wrong_preds, patterns)
        error_viz.savefig(self.output_dir / 'error_analysis.png', dpi=300, bbox_inches='tight')
        plt.close(error_viz)
        
        # ìƒ˜í”Œ ê°¤ëŸ¬ë¦¬
        if len(wrong_preds) > 0:
            gallery_fig = self.create_sample_gallery(wrong_preds, n_sample_images)
            if gallery_fig:
                gallery_fig.savefig(self.output_dir / 'wrong_predictions_gallery.png', dpi=300, bbox_inches='tight')
                plt.close(gallery_fig)
        
        # ì‹ ë¢°ë„ ë¶„ì„
        confidence_fig = self.create_confidence_analysis(df)
        confidence_fig.savefig(self.output_dir / 'confidence_analysis.png', dpi=300, bbox_inches='tight')
        plt.close(confidence_fig)
        
        # 5. HTML ë³´ê³ ì„œ ìƒì„±
        html_report = self.generate_html_report(wrong_preds, patterns, error_stats)
        
        # 6. JSON ê²°ê³¼ ì €ì¥
        analysis_results = {
            'error_stats': error_stats,
            'patterns': patterns,
            'wrong_predictions_summary': {
                'total_wrong': len(wrong_preds),
                'high_confidence_wrong': len(wrong_preds[wrong_preds['confidence'] > 0.8]),
                'low_confidence_wrong': len(wrong_preds[wrong_preds['confidence'] < 0.5])
            }
        }
        
        with open(self.output_dir / 'analysis_results.json', 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, indent=2, ensure_ascii=False)
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print("\nâœ… ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.output_dir}")
        print(f"ğŸ“„ HTML ë³´ê³ ì„œ: {html_report}")
        print(f"ğŸ“Š ì‹œê°í™” íŒŒì¼ë“¤:")
        print(f"   - error_analysis.png")
        print(f"   - wrong_predictions_gallery.png")
        print(f"   - confidence_analysis.png")
        
        return str(self.output_dir)


def main():
    """ë©”ì¸ í•¨ìˆ˜ - Fire CLI ì¸í„°í˜ì´ìŠ¤"""
    fire.Fire(WrongPredictionsExplorer)


if __name__ == "__main__":
    main()