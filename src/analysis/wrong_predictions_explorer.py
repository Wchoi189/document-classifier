"""
src/analysis/wrong_predictions_explorer.py

ì˜ëª»ëœ ì˜ˆì¸¡ íƒìƒ‰ê¸° - ì˜¤ë¶„ë¥˜ëœ ìƒ˜í”Œë“¤ì˜ ì‹œê°ì  ë¶„ì„ ë„êµ¬
Wrong predictions explorer - Visual analysis tool for misclassified samples
"""

import os
import sys
from pathlib import Path
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.figure import Figure
import seaborn as sns

from typing import List, Dict, Tuple, Optional
import json
from collections import defaultdict, Counter
import fire
from src.utils.config_utils import load_config, get_classification_metrics
from sklearn.metrics import classification_report, confusion_matrix


class WrongPredictionsExplorer:
    """ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ë¶„ì„ì„ ìœ„í•œ íƒìƒ‰ ë„êµ¬"""
    
    def __init__(self, config_path: str = "configs/config.yaml"):
        """
        ì´ˆê¸°í™”
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.config = load_config(config_path)
        self.setup_paths()
        self.load_class_info()
        
    def setup_paths(self):
        """ê²½ë¡œ ì„¤ì •"""
        self.data_dir = Path(self.config['data']['root_dir'])
        self.output_dir = Path('outputs/wrong_predictions_analysis')
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"âœ… ë°ì´í„° ë””ë ‰í† ë¦¬: {self.data_dir}")
        print(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥: {self.output_dir}")
    
    def load_class_info(self):
        """í´ë˜ìŠ¤ ì •ë³´ ë¡œë“œ"""
        meta_file = self.config['data']['meta_file']
        if os.path.exists(meta_file):
            self.meta_df = pd.read_csv(meta_file)
            self.class_names = dict(zip(self.meta_df['target'], self.meta_df['class_name']))
            print(f"âœ… í´ë˜ìŠ¤ ì •ë³´ ë¡œë“œ: {len(self.class_names)}ê°œ í´ë˜ìŠ¤")
        else:
            print(f"âš ï¸ ë©”íƒ€ íŒŒì¼ ì—†ìŒ, ê¸°ë³¸ í´ë˜ìŠ¤ëª… ì‚¬ìš©: {meta_file}")
            self.class_names = {i: f"class_{i}" for i in range(17)}
    
    def load_predictions(self, predictions_csv: str, ground_truth_csv: Optional[str] = None) -> pd.DataFrame:
        """ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ ë¡œë“œ"""
        print(f"ğŸ“¥ ì˜ˆì¸¡ ê²°ê³¼ ë¡œë“œ: {predictions_csv}")
        
        # ì˜ˆì¸¡ ê²°ê³¼ ë¡œë“œ
        df_pred = pd.read_csv(predictions_csv)
        required_cols = ['filename', 'predicted_target', 'confidence']
        
        if not all(col in df_pred.columns for col in required_cols):
            raise ValueError(f"ì˜ˆì¸¡ CSVì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {required_cols}")
        
        # ì •ë‹µ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
        if ground_truth_csv and os.path.exists(ground_truth_csv):
            print(f"ğŸ“¥ ì •ë‹µ ë°ì´í„° ë¡œë“œ: {ground_truth_csv}")
            df_true = pd.read_csv(ground_truth_csv)
            
            # íŒŒì¼ëª…ìœ¼ë¡œ ì¡°ì¸ (í™•ì¥ì ì œê±°)
            df_pred['join_key'] = df_pred['filename'].str.replace(r'\.(jpg|jpeg|png)$', '', regex=True)
            df_true['join_key'] = df_true['ID'].str.replace(r'\.(jpg|jpeg|png)$', '', regex=True)
            
            df_merged = pd.merge(df_pred, df_true, on='join_key', how='inner')
            
            if df_merged.empty:
                raise ValueError("ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ ë°ì´í„° ê°„ ë§¤ì¹­ë˜ëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            print(f"âœ… ë§¤ì¹­ëœ ìƒ˜í”Œ: {len(df_merged)}ê°œ")
            return df_merged
        else:
            print("âš ï¸ ì •ë‹µ ë°ì´í„° ì—†ìŒ - ì˜ˆì¸¡ ê²°ê³¼ë§Œ ë¶„ì„")
            df_pred['target'] = -1  # ë”ë¯¸ ì •ë‹µ
            return df_pred
    
    def identify_wrong_predictions(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
        """ì˜ëª»ëœ ì˜ˆì¸¡ ì‹ë³„ ë° ë¶„ì„"""
        if 'target' not in df.columns:
            print("âš ï¸ ì •ë‹µ ë¼ë²¨ì´ ì—†ì–´ ì˜¤ë¶„ë¥˜ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return df, {}
        
        # ì˜¤ë¶„ë¥˜ ì‹ë³„
        df['is_correct'] = df['predicted_target'] == df['target']
        df['error_type'] = df.apply(
            lambda row: f"{self.class_names.get(row['target'], row['target'])} â†’ {self.class_names.get(row['predicted_target'], row['predicted_target'])}" 
            if not row['is_correct'] else 'Correct', axis=1
        )
        
        wrong_preds = df[~df['is_correct']].copy()
        
        # ì˜¤ë¶„ë¥˜ í†µê³„
        error_stats = {
            'total_samples': len(df),
            'correct_predictions': df['is_correct'].sum(),
            'wrong_predictions': len(wrong_preds),
            'accuracy': df['is_correct'].mean(),
            'error_rate': 1 - df['is_correct'].mean()
        }
        
        print(f"ğŸ“Š ì˜¤ë¶„ë¥˜ ë¶„ì„ ê²°ê³¼:")
        print(f"   ì „ì²´ ìƒ˜í”Œ: {error_stats['total_samples']}")
        print(f"   ì •í™•í•œ ì˜ˆì¸¡: {error_stats['correct_predictions']}")
        print(f"   ì˜ëª»ëœ ì˜ˆì¸¡: {error_stats['wrong_predictions']}")
        print(f"   ì •í™•ë„: {error_stats['accuracy']:.3f}")
        
        return wrong_preds, error_stats
    
    def analyze_error_patterns(self, wrong_preds: pd.DataFrame) -> Dict:
        """ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„"""
        if wrong_preds.empty:
            return {}
        
        print("ğŸ” ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„ ì¤‘...")
        
        patterns = {}
        
        # 1. í´ë˜ìŠ¤ë³„ ì˜¤ë¶„ë¥˜ ë¹ˆë„
        patterns['class_errors'] = wrong_preds.groupby('target').size().to_dict()
        
        # 2. ê°€ì¥ ìì£¼ í˜¼ë™ë˜ëŠ” í´ë˜ìŠ¤ ìŒ
        error_pairs = wrong_preds.groupby(['target', 'predicted_target']).size().reset_index(name='count')
        patterns['confusion_pairs'] = error_pairs.nlargest(10, 'count').to_dict('records')
        
        # 3. ì‹ ë¢°ë„ë³„ ì˜¤ë¶„ë¥˜ ë¶„í¬
        confidence_bins = pd.cut(wrong_preds['confidence'], bins=[0, 0.5, 0.7, 0.9, 1.0], 
                               labels=['Very Low (0-0.5)', 'Low (0.5-0.7)', 'Medium (0.7-0.9)', 'High (0.9-1.0)'])
        patterns['confidence_distribution'] = confidence_bins.value_counts().to_dict()
        
        # 4. ë‚®ì€ ì‹ ë¢°ë„ ì˜ˆì¸¡ (ì„ê³„ê°’ ì´í•˜)
        low_confidence_threshold = 0.7
        patterns['low_confidence_errors'] = len(wrong_preds[wrong_preds['confidence'] < low_confidence_threshold])
        patterns['high_confidence_errors'] = len(wrong_preds[wrong_preds['confidence'] >= low_confidence_threshold])
        
        return patterns
    
    def create_error_visualization(self, wrong_preds: pd.DataFrame, patterns: Dict) -> Figure:
        """ì˜¤ë¥˜ ì‹œê°í™” ìƒì„±"""
        fig = plt.figure(figsize=(20, 15))
        
        # 1. í´ë˜ìŠ¤ë³„ ì˜¤ë¶„ë¥˜ ìˆ˜ (ì„œë¸Œí”Œë¡¯ 1)
        ax1 = plt.subplot(2, 3, 1)
        if patterns.get('class_errors'):
            class_error_series = pd.Series(patterns['class_errors'])
            class_error_series.index = pd.Index([self.class_names.get(idx, f"Class_{idx}") for idx in class_error_series.index])
            class_error_series.plot(kind='bar', ax=ax1)
            ax1.set_title('í´ë˜ìŠ¤ë³„ ì˜¤ë¶„ë¥˜ ê°œìˆ˜', fontsize=12)
            ax1.tick_params(axis='x', rotation=45)
        
        # 2. ì‹ ë¢°ë„ ë¶„í¬ (ì„œë¸Œí”Œë¡¯ 2)
        ax2 = plt.subplot(2, 3, 2)
        if not wrong_preds.empty:
            wrong_preds['confidence'].hist(bins=20, ax=ax2, alpha=0.7, color='red', label='Wrong')
            ax2.set_title('ì˜¤ë¶„ë¥˜ ì‹ ë¢°ë„ ë¶„í¬', fontsize=12)
            ax2.set_xlabel('Confidence Score')
            ax2.set_ylabel('Frequency')
        
        # 3. í˜¼ë™ ë§¤íŠ¸ë¦­ìŠ¤ (ì„œë¸Œí”Œë¡¯ 3-4, í° ì˜ì—­)
        ax3 = plt.subplot(2, 2, 2)
        if 'target' in wrong_preds.columns and not wrong_preds.empty:
            # ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ í˜¼ë™ ë§¤íŠ¸ë¦­ìŠ¤ í•„ìš” - ì—¬ê¸°ì„œëŠ” ì˜¤ë¶„ë¥˜ë§Œ í‘œì‹œ
            error_matrix = wrong_preds.groupby(['target', 'predicted_target']).size().unstack(fill_value=0)
            sns.heatmap(error_matrix, annot=True, fmt='d', cmap='Reds', ax=ax3)
            ax3.set_title('ì˜¤ë¶„ë¥˜ í˜¼ë™ ë§¤íŠ¸ë¦­ìŠ¤', fontsize=12)
        
        # 4. ê°€ì¥ í˜¼ë™ë˜ëŠ” í´ë˜ìŠ¤ ìŒ (ì„œë¸Œí”Œë¡¯ 5)
        ax4 = plt.subplot(2, 3, 5)
        if patterns.get('confusion_pairs'):
            pairs_df = pd.DataFrame(patterns['confusion_pairs'][:5])  # ìƒìœ„ 5ê°œ
            pairs_df['pair_label'] = pairs_df.apply(
                lambda row: f"{self.class_names.get(row['target'], row['target'])} â†’ {self.class_names.get(row['predicted_target'], row['predicted_target'])}", 
                axis=1
            )
            pairs_df.plot(x='pair_label', y='count', kind='bar', ax=ax4)
            ax4.set_title('ê°€ì¥ ìì£¼ í˜¼ë™ë˜ëŠ” í´ë˜ìŠ¤ ìŒ', fontsize=12)
            ax4.tick_params(axis='x', rotation=45)
        
        # 5. ì‹ ë¢°ë„ êµ¬ê°„ë³„ ì˜¤ë¶„ë¥˜ (ì„œë¸Œí”Œë¡¯ 6)
        ax5 = plt.subplot(2, 3, 6)
        if patterns.get('confidence_distribution'):
            conf_dist = pd.Series(patterns['confidence_distribution'])
            conf_dist.plot(kind='bar', ax=ax5, color='orange')
            ax5.set_title('ì‹ ë¢°ë„ êµ¬ê°„ë³„ ì˜¤ë¶„ë¥˜ ë¶„í¬', fontsize=12)
            ax5.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        return fig
    
    def create_sample_gallery(self, wrong_preds: pd.DataFrame, n_samples: int = 20) -> Figure:
        """ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ê°¤ëŸ¬ë¦¬ ìƒì„±"""
        if wrong_preds.empty:
            print("âš ï¸ í‘œì‹œí•  ì˜¤ë¶„ë¥˜ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ë‹¤ì–‘í•œ ì˜¤ë¥˜ ìœ í˜•ì—ì„œ ìƒ˜í”Œ ì„ íƒ
        samples_to_show = []
        
        # 1. ë†’ì€ ì‹ ë¢°ë„ ì˜¤ë¶„ë¥˜ (ëª¨ë¸ì´ í™•ì‹ í–ˆì§€ë§Œ í‹€ë¦° ê²½ìš°)
        high_conf_wrong = wrong_preds[wrong_preds['confidence'] > 0.8].head(5)
        samples_to_show.extend(high_conf_wrong.to_dict('records'))
        
        # 2. ë‚®ì€ ì‹ ë¢°ë„ ì˜¤ë¶„ë¥˜ (ëª¨ë¸ì´ ì• ë§¤í•´í–ˆë˜ ê²½ìš°)
        low_conf_wrong = wrong_preds[wrong_preds['confidence'] < 0.6].head(5)
        samples_to_show.extend(low_conf_wrong.to_dict('records'))
        
        # 3. ë‚˜ë¨¸ì§€ëŠ” ëœë¤ ìƒ˜í”Œë§
        remaining_samples = wrong_preds[~wrong_preds.index.isin([s['filename'] for s in samples_to_show if 'filename' in s])]
        if len(remaining_samples) > 0:
            random_samples = remaining_samples.sample(min(n_samples - len(samples_to_show), len(remaining_samples)))
            samples_to_show.extend(random_samples.to_dict('records'))
        
        # ì‹¤ì œ ì´ë¯¸ì§€ ë¡œë“œ ë° í‘œì‹œ
        n_cols = 5
        n_rows = (len(samples_to_show) + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 4 * n_rows))
        if n_rows == 1:
            axes = axes.reshape(1, -1)
        
        for i, sample in enumerate(samples_to_show):
            if i >= n_rows * n_cols:
                break
                
            row, col = i // n_cols, i % n_cols
            
            # ì´ë¯¸ì§€ ë¡œë“œ
            img_path = self.data_dir / 'train' / sample['filename']
            if not img_path.exists():
                img_path = self.data_dir / 'test' / sample['filename']
            
            if img_path.exists():
                img = cv2.imread(str(img_path))
                if img is not None:
                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    axes[row, col].imshow(img)
                    
                    # ì œëª© ì„¤ì •
                    true_class = self.class_names.get(sample.get('target', -1), 'Unknown')
                    pred_class = self.class_names.get(sample['predicted_target'], 'Unknown')
                    conf = sample['confidence']
                    
                    title = f"ì‹¤ì œ: {true_class}\nì˜ˆì¸¡: {pred_class}\nì‹ ë¢°ë„: {conf:.3f}"
                    axes[row, col].set_title(title, fontsize=10)
                else:
                    axes[row, col].text(0.5, 0.5, 'Image\nLoad\nError', 
                                      ha='center', va='center', transform=axes[row, col].transAxes)
            else:
                axes[row, col].text(0.5, 0.5, 'Image\nNot\nFound', 
                                  ha='center', va='center', transform=axes[row, col].transAxes)
            
            axes[row, col].axis('off')
        
        # ë¹ˆ subplotë“¤ ìˆ¨ê¸°ê¸°
        for i in range(len(samples_to_show), n_rows * n_cols):
            row, col = i // n_cols, i % n_cols
            axes[row, col].axis('off')
        
        plt.suptitle('ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ê°¤ëŸ¬ë¦¬', fontsize=16)
        plt.tight_layout()
        return fig
    
    def create_confidence_analysis(self, df: pd.DataFrame) -> Dict:
        """ì‹ ë¢°ë„ ê¸°ë°˜ ë¶„ì„"""
        analysis = {}
        
        if df.empty:
            return analysis
        
        # ì‹ ë¢°ë„ êµ¬ê°„ë³„ ì •í™•ë„
        confidence_ranges = [
            (0.0, 0.5, "ë§¤ìš° ë‚®ìŒ"),
            (0.5, 0.7, "ë‚®ìŒ"), 
            (0.7, 0.9, "ë³´í†µ"),
            (0.9, 1.0, "ë†’ìŒ")
        ]
        
        range_analysis = {}
        for min_conf, max_conf, label in confidence_ranges:
            mask = (df['confidence'] >= min_conf) & (df['confidence'] < max_conf)
            subset = df[mask]
            
            if len(subset) > 0 and 'is_correct' in subset.columns:
                range_analysis[label] = {
                    'count': len(subset),
                    'accuracy': subset['is_correct'].mean(),
                    'avg_confidence': subset['confidence'].mean()
                }
            else:
                range_analysis[label] = {
                    'count': len(subset),
                    'accuracy': None,
                    'avg_confidence': subset['confidence'].mean() if len(subset) > 0 else 0
                }
        
        analysis['confidence_ranges'] = range_analysis
        
        # ì‹ ë¢°ë„ ì„ê³„ê°’ë³„ ì„±ëŠ¥
        thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]
        threshold_analysis = {}
        
        for threshold in thresholds:
            high_conf = df[df['confidence'] >= threshold]
            if len(high_conf) > 0 and 'is_correct' in high_conf.columns:
                threshold_analysis[threshold] = {
                    'samples_above_threshold': len(high_conf),
                    'accuracy_above_threshold': high_conf['is_correct'].mean(),
                    'coverage': len(high_conf) / len(df)
                }
        
        analysis['threshold_analysis'] = threshold_analysis
        return analysis
    
    def generate_html_report(self, wrong_preds: pd.DataFrame, patterns: Dict, 
                           confidence_analysis: Dict, output_filename: Optional[str] = None):
        """HTML ë³´ê³ ì„œ ìƒì„±"""
        if output_filename is None:
            output_filename = "wrong_predictions_report.html"
        
        html_path = self.output_dir / output_filename
        
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>ì˜¤ë¶„ë¥˜ ë¶„ì„ ë³´ê³ ì„œ</title>
            <meta charset="utf-8">
            <style>
                body {{ font-family: Arial, sans-serif; margin: 40px; }}
                .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
                .section {{ margin: 30px 0; }}
                .metric {{ display: inline-block; margin: 10px; padding: 15px; 
                          background-color: #e8f4f8; border-radius: 5px; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
                .error-pair {{ margin: 5px 0; padding: 10px; background-color: #fff2f2; 
                             border-left: 4px solid #ff4444; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>ğŸ” ì˜¤ë¶„ë¥˜ ë¶„ì„ ë³´ê³ ì„œ</h1>
                <p>ëª¨ë¸ì˜ ì˜ëª»ëœ ì˜ˆì¸¡ì— ëŒ€í•œ ìƒì„¸ ë¶„ì„</p>
            </div>
        """
        
        # ê¸°ë³¸ í†µê³„
        if patterns:
            html_content += f"""
            <div class="section">
                <h2>ğŸ“Š ê¸°ë³¸ í†µê³„</h2>
                <div class="metric">
                    <strong>ì „ì²´ ì˜¤ë¶„ë¥˜:</strong> {patterns.get('wrong_predictions', 0)}ê°œ
                </div>
                <div class="metric">
                    <strong>ì˜¤ë¶„ë¥˜ìœ¨:</strong> {patterns.get('error_rate', 0):.3f}
                </div>
                <div class="metric">
                    <strong>ë†’ì€ ì‹ ë¢°ë„ ì˜¤ë¶„ë¥˜:</strong> {patterns.get('high_confidence_errors', 0)}ê°œ
                </div>
                <div class="metric">
                    <strong>ë‚®ì€ ì‹ ë¢°ë„ ì˜¤ë¶„ë¥˜:</strong> {patterns.get('low_confidence_errors', 0)}ê°œ
                </div>
            </div>
            """
        
        # ê°€ì¥ í˜¼ë™ë˜ëŠ” í´ë˜ìŠ¤ ìŒ
        if patterns.get('confusion_pairs'):
            html_content += """
            <div class="section">
                <h2>ğŸ”„ ê°€ì¥ í˜¼ë™ë˜ëŠ” í´ë˜ìŠ¤ ìŒ</h2>
            """
            for pair in patterns['confusion_pairs'][:10]:
                true_class = self.class_names.get(pair['target'], f"Class_{pair['target']}")
                pred_class = self.class_names.get(pair['predicted_target'], f"Class_{pair['predicted_target']}")
                html_content += f"""
                <div class="error-pair">
                    <strong>{true_class}</strong> â†’ <strong>{pred_class}</strong>: {pair['count']}íšŒ
                </div>
                """
            html_content += "</div>"
        
        # ì‹ ë¢°ë„ ë¶„ì„
        if confidence_analysis.get('confidence_ranges'):
            html_content += """
            <div class="section">
                <h2>ğŸ“ˆ ì‹ ë¢°ë„ êµ¬ê°„ë³„ ë¶„ì„</h2>
                <table>
                    <tr><th>ì‹ ë¢°ë„ êµ¬ê°„</th><th>ìƒ˜í”Œ ìˆ˜</th><th>ì •í™•ë„</th><th>í‰ê·  ì‹ ë¢°ë„</th></tr>
            """
            for range_name, data in confidence_analysis['confidence_ranges'].items():
                accuracy_str = f"{data['accuracy']:.3f}" if data['accuracy'] is not None else "N/A"
                html_content += f"""
                <tr>
                    <td>{range_name}</td>
                    <td>{data['count']}</td>
                    <td>{accuracy_str}</td>
                    <td>{data['avg_confidence']:.3f}</td>
                </tr>
                """
            html_content += "</table></div>"
        
        html_content += """
        </body>
        </html>
        """
        
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"ğŸ“„ HTML ë³´ê³ ì„œ ìƒì„±: {html_path}")
        return str(html_path)
    
    def generate_comprehensive_analysis(self, predictions_csv: str, ground_truth_csv: Optional[str] = None):
        """ì¢…í•© ì˜¤ë¶„ë¥˜ ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ ì¢…í•© ì˜¤ë¶„ë¥˜ ë¶„ì„ ì‹œì‘...")
        
        # 1. ë°ì´í„° ë¡œë“œ
        df = self.load_predictions(predictions_csv, ground_truth_csv)
        
        # 2. ì˜¤ë¶„ë¥˜ ì‹ë³„
        wrong_preds, error_stats = self.identify_wrong_predictions(df)
        
        if wrong_preds.empty:
            print("âœ… ì˜¤ë¶„ë¥˜ê°€ ì—†ê±°ë‚˜ ì •ë‹µ ë°ì´í„°ê°€ ì—†ì–´ ë¶„ì„ì„ ì™„ë£Œí•©ë‹ˆë‹¤.")
            return
        
        # 3. ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„
        patterns = self.analyze_error_patterns(wrong_preds)
        patterns.update(error_stats)
        
        # 4. ì‹ ë¢°ë„ ë¶„ì„
        confidence_analysis = self.create_confidence_analysis(df)
        
        # 5. ì‹œê°í™” ìƒì„±
        print("ğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")
        error_viz = self.create_error_visualization(wrong_preds, patterns)
        viz_path = self.output_dir / 'error_analysis_visualization.png'
        error_viz.savefig(viz_path, dpi=300, bbox_inches='tight')
        plt.close(error_viz)
        
        gallery_path = None
        # 6. ìƒ˜í”Œ ê°¤ëŸ¬ë¦¬ ìƒì„±
        print("ğŸ–¼ï¸ ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ê°¤ëŸ¬ë¦¬ ìƒì„± ì¤‘...")
        gallery_fig = self.create_sample_gallery(wrong_preds, n_samples=20)
        if gallery_fig:
            gallery_path = self.output_dir / 'wrong_predictions_gallery.png'
            gallery_fig.savefig(gallery_path, dpi=300, bbox_inches='tight')
            plt.close(gallery_fig)
        
        # 7. HTML ë³´ê³ ì„œ ìƒì„±
        print("ğŸ“„ HTML ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        html_path = self.generate_html_report(wrong_preds, patterns, confidence_analysis)
        
        # 8. JSON ë¶„ì„ ê²°ê³¼ ì €ì¥
        def convert_np(obj):
            if isinstance(obj, dict):
                return {convert_np(k): convert_np(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_np(i) for i in obj]
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, (np.floating,)):
                return float(obj)
            else:
                return obj

        analysis_results = {
            'error_patterns': patterns,
            'confidence_analysis': confidence_analysis,
            'summary': {
                'total_wrong_predictions': int(len(wrong_preds)),
                'most_confused_classes': convert_np(patterns.get('confusion_pairs', [])[:5]),
                'recommendations': self._generate_recommendations(patterns, confidence_analysis)
            }
        }
        analysis_results = convert_np(analysis_results)
        
        json_path = self.output_dir / 'analysis_results.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, indent=2, ensure_ascii=False)
        
        print("\nâœ… ì¢…í•© ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“Š ì‹œê°í™”: {viz_path}")
        print(f"ğŸ–¼ï¸ ê°¤ëŸ¬ë¦¬: {gallery_path if 'gallery_path' is not None in locals() else 'N/A'}")
        print(f"ğŸ“„ HTML ë³´ê³ ì„œ: {html_path}")
        print(f"ğŸ“‹ JSON ê²°ê³¼: {json_path}")
        
        return {
            'visualization': str(viz_path),
            'gallery': str(gallery_path) if 'gallery_path' in locals() else None,
            'html_report': html_path,
            'json_results': str(json_path)
        }
    
    def _generate_recommendations(self, patterns: Dict, confidence_analysis: Dict) -> List[str]:
        """ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ë†’ì€ ì‹ ë¢°ë„ ì˜¤ë¶„ë¥˜ê°€ ë§ì€ ê²½ìš°
        if patterns.get('high_confidence_errors', 0) > patterns.get('low_confidence_errors', 0):
            recommendations.append("ëª¨ë¸ì´ í™•ì‹ ì„ ê°€ì§€ê³  í‹€ë¦¬ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ë°ì´í„° í’ˆì§ˆì´ë‚˜ ë¼ë²¨ë§ì„ ì¬ê²€í† í•´ë³´ì„¸ìš”.")
        
        # íŠ¹ì • í´ë˜ìŠ¤ì—ì„œ ì˜¤ë¶„ë¥˜ê°€ ì§‘ì¤‘ëœ ê²½ìš°
        if patterns.get('class_errors'):
            max_errors = max(patterns['class_errors'].values())
            total_errors = sum(patterns['class_errors'].values())
            if max_errors > total_errors * 0.3:  # 30% ì´ìƒì´ í•œ í´ë˜ìŠ¤ì— ì§‘ì¤‘
                recommendations.append("íŠ¹ì • í´ë˜ìŠ¤ì—ì„œ ì˜¤ë¶„ë¥˜ê°€ ì§‘ì¤‘ë˜ê³  ìˆìŠµë‹ˆë‹¤. í•´ë‹¹ í´ë˜ìŠ¤ì˜ ë°ì´í„° ì¦ê°•ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        
        # ë‚®ì€ ì‹ ë¢°ë„ ì˜ˆì¸¡ì´ ë§ì€ ê²½ìš°
        if patterns.get('low_confidence_errors', 0) > patterns.get('high_confidence_errors', 0):
            recommendations.append("ëª¨ë¸ì´ í™•ì‹ ì´ ì—†ëŠ” ì˜ˆì¸¡ì´ ë§ìŠµë‹ˆë‹¤. ëª¨ë¸ ë³µì¡ë„ë¥¼ ë†’ì´ê±°ë‚˜ ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # í˜¼ë™ë˜ëŠ” í´ë˜ìŠ¤ ìŒì´ ëª…í™•í•œ ê²½ìš°
        if patterns.get('confusion_pairs'):
            top_confusion = patterns['confusion_pairs'][0]
            if top_confusion['count'] > 5:  # 5íšŒ ì´ìƒ í˜¼ë™
                recommendations.append(f"í´ë˜ìŠ¤ ê°„ í˜¼ë™ì´ ìì£¼ ë°œìƒí•©ë‹ˆë‹¤. ìœ ì‚¬í•œ í´ë˜ìŠ¤ë“¤ì˜ êµ¬ë¶„ íŠ¹ì§•ì„ ê°•í™”í•˜ëŠ” ì¦ê°• ê¸°ë²•ì„ ì ìš©í•´ë³´ì„¸ìš”.")
        
        return recommendations


def main():
    """ë©”ì¸ í•¨ìˆ˜ - Fire CLI ì¸í„°í˜ì´ìŠ¤"""
    fire.Fire(WrongPredictionsExplorer)


if __name__ == "__main__":
    main()