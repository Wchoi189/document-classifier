# ğŸ”§ FINAL FIXED: conservative_augmentation_tester.py
# í•µì‹¬ ìˆ˜ì •: Enhanced config loadingìœ¼ë¡œ Hydra defaults ë¬¸ì œ í•´ê²°

import sys
import os
from pathlib import Path

# ğŸ”§ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì • (í•­ìƒ ì²« ë²ˆì§¸ë¡œ)
project_root = str(Path(__file__).parent.parent.parent)
sys.path.insert(0, project_root)
os.chdir(project_root)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
import json
import fire
from icecream import ic
import copy
from datetime import datetime

# ğŸ”§ FIXED: Enhanced config loading ì‚¬ìš©
from src.utils.config_utils import load_config, normalize_config_structure, safe_config_get, load_config_legacy
from src.data.csv_dataset import CSVDocumentDataset
from src.data.augmentation import get_configurable_transforms, get_valid_transforms
from src.models.model import create_model
from src.trainer.trainer import Trainer


class ConservativeAugmentationTester:
    """ê¸°ì¡´ ì„±ëŠ¥ ìœ ì§€í•˜ë©° ì•ˆì „í•˜ê²Œ ì¦ê°• í…ŒìŠ¤íŠ¸í•˜ëŠ” ì‹œìŠ¤í…œ"""
    
    def __init__(self, config_path: str = "configs/config.yaml", 
                 baseline_checkpoint: str = None):
        """
        ì´ˆê¸°í™”
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
            baseline_checkpoint: ê¸°ì¤€ì ì´ ë˜ëŠ” ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
        """
        ic("ğŸ”§ Enhanced config loading ì‚¬ìš©")
        self.config = load_config(config_path)
        self.config = normalize_config_structure(self.config)
        self.baseline_checkpoint = baseline_checkpoint
        self.setup_paths()
        
        # ğŸ”§ DEBUG: Config êµ¬ì¡° í™•ì¸
        ic("ğŸ” Config êµ¬ì¡° ê²€ì¦")
        self._verify_config_completeness()
        
        ic("ë³´ìˆ˜ì  ì¦ê°• í…ŒìŠ¤í„° ì´ˆê¸°í™” ì™„ë£Œ")
        
    def _verify_config_completeness(self):
        """ğŸ”§ Config ì™„ì„±ë„ ê²€ì¦"""
        ic("ì „ì²´ config í‚¤:", list(self.config.keys()))
        
        required_sections = {
            'model': ['name', 'pretrained'],
            'optimizer': ['name', 'learning_rate', 'weight_decay'],
            'train': ['epochs', 'batch_size'],
            'data': ['root_dir', 'csv_file', 'meta_file']
        }
        
        for section, required_keys in required_sections.items():
            if section in self.config:
                ic(f"âœ… {section} ì„¹ì…˜ ì¡´ì¬")
                section_config = self.config[section]
                for key in required_keys:
                    if key in section_config:
                        ic(f"  âœ… {section}.{key} ì¡´ì¬")
                    else:
                        ic(f"  âš ï¸ {section}.{key} ì—†ìŒ")
            else:
                ic(f"âŒ {section} ì„¹ì…˜ ì—†ìŒ")
    
    def setup_paths(self):
        """ê²½ë¡œ ì„¤ì •"""
        self.output_dir = Path('outputs/conservative_augmentation_test')
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ì‹¤í—˜ ê²°ê³¼ ì €ì¥ìš© ë””ë ‰í† ë¦¬
        self.experiment_dir = self.output_dir / f"experiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.experiment_dir.mkdir(parents=True, exist_ok=True)
        
        ic(f"ì‹¤í—˜ ê²°ê³¼ ì €ì¥: {self.experiment_dir}")
    
    def create_progressive_augmentation_configs(self) -> List[Dict]:
        """ì ì§„ì  ì¦ê°• ì„¤ì •ë“¤ ìƒì„± (Gemini ì „ëµ ê¸°ë°˜)"""
        ic("ì ì§„ì  ì¦ê°• ì„¤ì • ìƒì„±")
        
        # ê¸°ë³¸ ì„¤ì • ë³µì‚¬
        base_config = copy.deepcopy(self.config)
        
        # Geminiì˜ ê¶Œì¥ì‚¬í•­ ê¸°ë°˜ ì ì§„ì  ì„¤ì •
        progressive_configs = []
        
        # Phase 0: ê¸°ì¤€ì  (ê¸°ì¡´ ì„¤ì •)
        baseline_config = copy.deepcopy(base_config)
        baseline_config['experiment_name'] = 'phase_0_baseline'
        baseline_config['description'] = 'ê¸°ì¤€ì  - ê¸°ì¡´ ì„¤ì •'
        baseline_config['data']['augmentation'] = {
            'strategy': 'basic',
            'intensity': 0.3
        }
        progressive_configs.append(baseline_config)
        
        # Phase 1: íšŒì „ ë„ì… (Critical Priority)
        phase1_config = copy.deepcopy(base_config)
        phase1_config['experiment_name'] = 'phase_1_rotation_mild'
        phase1_config['description'] = 'Phase 1: íšŒì „ ì¦ê°• ë„ì… (Â±10Â°)'
        phase1_config['data']['augmentation'] = {
            'strategy': 'robust',
            'intensity': 0.5
        }
        progressive_configs.append(phase1_config)
        
        # Phase 2: íšŒì „ + ì¡°ëª… ê°•í™”
        phase2_config = copy.deepcopy(base_config)
        phase2_config['experiment_name'] = 'phase_2_rotation_lighting'
        phase2_config['description'] = 'Phase 2: íšŒì „ + ì¡°ëª… ì¦ê°• (Â±15Â°, ê³¼ë…¸ì¶œ ì‹œë®¬ë ˆì´ì…˜)'
        phase2_config['data']['augmentation'] = {
            'strategy': 'robust',
            'intensity': 0.6
        }
        progressive_configs.append(phase2_config)
        
        # Phase 3: íƒ€ê²Ÿ ê°•ë„ (Â±25Â°)
        phase3_config = copy.deepcopy(base_config)
        phase3_config['experiment_name'] = 'phase_3_target_intensity'
        phase3_config['description'] = 'Phase 3: íƒ€ê²Ÿ ê°•ë„ (Â±25Â°, í’€ ì¡°ëª… ì¦ê°•)'
        phase3_config['data']['augmentation'] = {
            'strategy': 'robust',
            'intensity': 0.7
        }
        progressive_configs.append(phase3_config)
        
        # Phase 4: ìµœëŒ€ ê°•ë„ (Â±45Â° - Gemini ìµœì¢… ê¶Œì¥)
        phase4_config = copy.deepcopy(base_config)
        phase4_config['experiment_name'] = 'phase_4_maximum_robustness'
        phase4_config['description'] = 'Phase 4: ìµœëŒ€ robustness (Â±45Â°)'
        phase4_config['data']['augmentation'] = {
            'strategy': 'robust',
            'intensity': 0.8
        }
        progressive_configs.append(phase4_config)
        
        ic(f"ìƒì„±ëœ ì¦ê°• ì„¤ì •: {len(progressive_configs)}ê°œ ë‹¨ê³„")
        return progressive_configs
    
    def create_augmentation_transforms(self, aug_config: Dict):
        """ì¦ê°• ì„¤ì •ìœ¼ë¡œë¶€í„° transform ìƒì„±"""
        data_config = safe_config_get(self.config, 'data', {})
        img_size = data_config.get('image_size', 224)
        mean = data_config.get('mean', [0.485, 0.456, 0.406])
        std = data_config.get('std', [0.229, 0.224, 0.225])
        
        # ì„¤ì •ëœ ì¦ê°• ì „ëµì— ë”°ë¼ transform ìƒì„±
        if aug_config.get('strategy') == 'robust':
            return get_configurable_transforms(img_size, img_size, mean, std, aug_config)
        else:
            # ê¸°ë³¸ ì¦ê°•ìœ¼ë¡œ í´ë°±
            from src.data.augmentation import get_train_transforms
            return get_train_transforms(img_size, img_size, mean, std)
    
    def run_quick_validation(self, config: Dict, epochs: int = 3) -> Dict:
        """ë¹ ë¥¸ ê²€ì¦ í›ˆë ¨ (ì„±ëŠ¥ ë³€í™” í™•ì¸ìš©)"""
        ic(f"ë¹ ë¥¸ ê²€ì¦ ì‹œì‘: {config['experiment_name']}")
        
        # Initialize augmentation_config before try block
        augmentation_config = config.get('data', {}).get('augmentation', {})
        
        try:
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            device = torch.device(self.config.get('device', 'cuda') if torch.cuda.is_available() else 'cpu')
            
            # ğŸ”§ Config ì ‘ê·¼ - ì´ì œ ëª¨ë“  í‚¤ê°€ ì¡´ì¬í•¨
            data_config = self.config['data']
            train_config = self.config['train']
            model_config = self.config['model']
            optimizer_config = self.config['optimizer']
            
            # ë°ì´í„° ì¤€ë¹„
            # augmentation_config already initialized above
            
            # Transform ìƒì„±
            train_transforms = self.create_augmentation_transforms(augmentation_config)
            valid_transforms = get_valid_transforms(
                height=data_config['image_size'],
                width=data_config['image_size'],
                mean=data_config['mean'],
                std=data_config['std']
            )
            
            # ë°ì´í„°ì…‹ ìƒì„±
            train_dataset = CSVDocumentDataset(
                root_dir=data_config['root_dir'],
                csv_file=data_config['csv_file'],
                meta_file=data_config['meta_file'],
                split='train',
                transform=train_transforms,
                val_size=data_config['val_size'],
                seed=self.config['seed']
            )
            
            val_dataset = CSVDocumentDataset(
                root_dir=data_config['root_dir'],
                csv_file=data_config['csv_file'],
                meta_file=data_config['meta_file'],
                split='val',
                transform=valid_transforms,
                val_size=data_config['val_size'],
                seed=self.config['seed']
            )
            
            # ë°ì´í„° ë¡œë” ìƒì„±
            batch_size = train_config['batch_size']
            num_workers = data_config['num_workers']
            
            train_loader = DataLoader(
                train_dataset,
                batch_size=batch_size,
                shuffle=True,
                num_workers=num_workers,
                pin_memory=True if device.type == 'cuda' else False
            )
            
            val_loader = DataLoader(
                val_dataset,
                batch_size=batch_size,
                shuffle=False,
                num_workers=num_workers,
                pin_memory=True if device.type == 'cuda' else False
            )
            
            # ëª¨ë¸ ìƒì„±
            num_classes = len(train_dataset.classes)
            model_name = model_config['name']
            pretrained = model_config['pretrained']
            
            model = create_model(
                model_name=model_name,
                num_classes=num_classes,
                pretrained=pretrained
            ).to(device)
            
            # ê¸°ì¤€ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
            if self.baseline_checkpoint and Path(self.baseline_checkpoint).exists():
                ic(f"ê¸°ì¤€ ëª¨ë¸ ë¡œë“œ: {self.baseline_checkpoint}")
                checkpoint = torch.load(self.baseline_checkpoint, map_location=device)
                model.load_state_dict(checkpoint)
            
            # ğŸ”§ ì˜µí‹°ë§ˆì´ì € ì„¤ì • - ì´ì œ ì•ˆì „í•˜ê²Œ ì ‘ê·¼ ê°€ëŠ¥
            if optimizer_config['name'] == 'AdamW':
                optimizer = optim.AdamW(
                    model.parameters(),
                    lr=optimizer_config['learning_rate'],
                    weight_decay=optimizer_config['weight_decay']
                )
            elif optimizer_config['name'] == 'Adam':
                optimizer = optim.Adam(
                    model.parameters(),
                    lr=optimizer_config['learning_rate'],
                    weight_decay=optimizer_config['weight_decay']
                )
            else:
                optimizer = optim.SGD(
                    model.parameters(),
                    lr=optimizer_config['learning_rate'],
                    weight_decay=optimizer_config['weight_decay']
                )
            
            # ì†ì‹¤ í•¨ìˆ˜
            loss_fn = nn.CrossEntropyLoss()
            
            # ë¹ ë¥¸ í›ˆë ¨ì„ ìœ„í•œ ì„¤ì • ìˆ˜ì •
            quick_config = copy.deepcopy(config)
            quick_config['train']['epochs'] = epochs
            if 'wandb' in quick_config:
                quick_config['wandb']['enabled'] = False  # WandB ë¡œê¹… ë¹„í™œì„±í™”
            
            # íŠ¸ë ˆì´ë„ˆ ìƒì„± (WandB ì—†ëŠ” ê¸°ë³¸ íŠ¸ë ˆì´ë„ˆ)
            trainer = Trainer(model, optimizer, None, loss_fn, train_loader, val_loader, device, quick_config)
            
            # í›ˆë ¨ ì‹¤í–‰
            trainer.train()
            
            # ê²°ê³¼ ìˆ˜ì§‘
            if trainer.history:
                final_results = trainer.history[-1]
                results = {
                    'experiment_name': config['experiment_name'],
                    'final_val_accuracy': final_results.get('val_acc', 0),
                    'final_val_f1': final_results.get('val_f1', 0),
                    'final_train_loss': final_results.get('train_loss', 0),
                    'final_val_loss': final_results.get('val_loss', 0),
                    'epochs_completed': len(trainer.history),
                    'augmentation_config': augmentation_config
                }
            else:
                results = {
                    'experiment_name': config['experiment_name'],
                    'error': 'No training history available',
                    'augmentation_config': augmentation_config
                }
            
            # ëª¨ë¸ ì €ì¥
            model_path = self.experiment_dir / f"{config['experiment_name']}_model.pth"
            torch.save(model.state_dict(), model_path)
            results['model_path'] = str(model_path)
            
            ic(f"ë¹ ë¥¸ ê²€ì¦ ì™„ë£Œ: {config['experiment_name']}")
            ic(f"Val F1: {results.get('final_val_f1', 0):.4f}")
            
            return results
            
        except Exception as e:
            ic(f"ì˜¤ë¥˜ ë°œìƒ: {config['experiment_name']} - {e}")
            import traceback
            ic("ìƒì„¸ ì˜¤ë¥˜:", traceback.format_exc())
            return {
                'experiment_name': config['experiment_name'],
                'error': str(e),
                'augmentation_config': augmentation_config if 'augmentation_config' in locals() else {}
            }
    
    def compare_with_baseline(self, results: List[Dict]) -> Dict:
        """ê¸°ì¤€ì  ëŒ€ë¹„ ì„±ëŠ¥ ë¹„êµ"""
        ic("ê¸°ì¤€ì  ëŒ€ë¹„ ì„±ëŠ¥ ë¹„êµ")
        
        baseline_result = None
        for result in results:
            if 'baseline' in result['experiment_name']:
                baseline_result = result
                break
        
        if not baseline_result:
            ic("ê¸°ì¤€ì  ê²°ê³¼ ì—†ìŒ")
            return {}
        
        baseline_f1 = baseline_result.get('final_val_f1', 0)
        baseline_acc = baseline_result.get('final_val_accuracy', 0)
        
        comparison = {
            'baseline_performance': {
                'f1': baseline_f1,
                'accuracy': baseline_acc
            },
            'phase_comparisons': []
        }
        
        for result in results:
            if result['experiment_name'] == baseline_result['experiment_name']:
                continue
            
            current_f1 = result.get('final_val_f1', 0)
            current_acc = result.get('final_val_accuracy', 0)
            
            f1_improvement = current_f1 - baseline_f1
            acc_improvement = current_acc - baseline_acc
            
            phase_comparison = {
                'experiment_name': result['experiment_name'],
                'f1_score': current_f1,
                'accuracy': current_acc,
                'f1_improvement': f1_improvement,
                'f1_improvement_percent': (f1_improvement / baseline_f1 * 100) if baseline_f1 > 0 else 0,
                'accuracy_improvement': acc_improvement,
                'performance_safe': f1_improvement >= -0.05,  # 5% ì´í•˜ ì„±ëŠ¥ ì €í•˜ë§Œ í—ˆìš©
                'recommended_for_full_training': f1_improvement > 0.02 and f1_improvement >= -0.02
            }
            
            comparison['phase_comparisons'].append(phase_comparison)
        
        # ìµœê³  ì„±ëŠ¥ ë‹¨ê³„ ì‹ë³„
        best_phase = max(comparison['phase_comparisons'], 
                        key=lambda x: x['f1_score']) if comparison['phase_comparisons'] else None
        
        if best_phase:
            comparison['best_phase'] = best_phase['experiment_name']
            comparison['best_f1_improvement'] = best_phase['f1_improvement']
        
        return comparison
    
    def generate_recommendations(self, comparison: Dict) -> Dict:
        """ë¹„êµ ê²°ê³¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        ic("ê¶Œì¥ì‚¬í•­ ìƒì„±")
        
        recommendations = {
            'safe_phases': [],
            'promising_phases': [],
            'risky_phases': [],
            'next_actions': [],
            'full_training_candidate': None
        }
        
        if 'phase_comparisons' not in comparison:
            return recommendations
        
        for phase in comparison['phase_comparisons']:
            if phase['performance_safe']:
                recommendations['safe_phases'].append(phase['experiment_name'])
                
                if phase['recommended_for_full_training']:
                    recommendations['promising_phases'].append(phase['experiment_name'])
            else:
                recommendations['risky_phases'].append(phase['experiment_name'])
        
        # í’€ í›ˆë ¨ í›„ë³´ ì„ ì •
        promising_phases = [p for p in comparison['phase_comparisons'] 
                          if p['recommended_for_full_training']]
        
        if promising_phases:
            best_promising = max(promising_phases, key=lambda x: x['f1_improvement'])
            recommendations['full_training_candidate'] = best_promising['experiment_name']
            
            recommendations['next_actions'].extend([
                f"'{best_promising['experiment_name']}' ì„¤ì •ìœ¼ë¡œ í’€ í›ˆë ¨ ì‹¤í–‰ ê¶Œì¥",
                f"ì˜ˆìƒ F1 ê°œì„ : {best_promising['f1_improvement']:.4f} ({best_promising['f1_improvement_percent']:.1f}%)",
                "ê¸°ì¡´ 79% ì„±ëŠ¥ ëŒ€ë¹„ ì•ˆì „í•œ ê°œì„  ì˜ˆìƒ"
            ])
        else:
            recommendations['next_actions'].extend([
                "í˜„ì¬ ë‹¨ê³„ì—ì„œëŠ” í° ê°œì„ ì´ ì–´ë ¤ì›€",
                "ë” ì„¸ë°€í•œ ì¦ê°• íŠœë‹ ë˜ëŠ” ë‹¤ë¥¸ ì ‘ê·¼ë²• í•„ìš”",
                "ê¸°ì¡´ ì„¤ì • ìœ ì§€ë¥¼ ê¶Œì¥"
            ])
        
        return recommendations
    
    def save_experiment_results(self, results: List[Dict], comparison: Dict, 
                              recommendations: Dict):
        """ì‹¤í—˜ ê²°ê³¼ ì €ì¥"""
        ic("ì‹¤í—˜ ê²°ê³¼ ì €ì¥")
        
        # ì „ì²´ ê²°ê³¼ ì €ì¥
        all_results = {
            'timestamp': datetime.now().isoformat(),
            'baseline_checkpoint': self.baseline_checkpoint,
            'phase_results': results,
            'performance_comparison': comparison,
            'recommendations': recommendations
        }
        
        with open(self.experiment_dir / 'experiment_results.json', 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        
        # ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
        report_path = self.experiment_dir / 'experiment_summary.txt'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# ë³´ìˆ˜ì  ì¦ê°• í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë³´ê³ ì„œ\n")
            f.write("=" * 50 + "\n\n")
            
            # ê¸°ì¤€ì  ì„±ëŠ¥
            if 'baseline_performance' in comparison:
                baseline = comparison['baseline_performance']
                f.write(f"## ê¸°ì¤€ì  ì„±ëŠ¥\n")
                f.write(f"- F1 Score: {baseline['f1']:.4f}\n")
                f.write(f"- Accuracy: {baseline['accuracy']:.4f}\n\n")
            
            # ë‹¨ê³„ë³„ ê²°ê³¼
            f.write("## ë‹¨ê³„ë³„ ê²°ê³¼\n")
            for phase in comparison.get('phase_comparisons', []):
                f.write(f"### {phase['experiment_name']}\n")
                f.write(f"- F1 Score: {phase['f1_score']:.4f} (ê°œì„ : {phase['f1_improvement']:+.4f})\n")
                f.write(f"- Accuracy: {phase['accuracy']:.4f} (ê°œì„ : {phase['accuracy_improvement']:+.4f})\n")
                f.write(f"- ì•ˆì „ì„±: {'âœ…' if phase['performance_safe'] else 'âŒ'}\n")
                f.write(f"- í’€ í›ˆë ¨ ê¶Œì¥: {'âœ…' if phase['recommended_for_full_training'] else 'âŒ'}\n\n")
            
            # ê¶Œì¥ì‚¬í•­
            f.write("## ê¶Œì¥ì‚¬í•­\n")
            for action in recommendations.get('next_actions', []):
                f.write(f"- {action}\n")
            
            if recommendations.get('full_training_candidate'):
                f.write(f"\n**ì¶”ì²œ ì„¤ì •**: {recommendations['full_training_candidate']}\n")
        
        ic(f"ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {self.experiment_dir}")
        return str(report_path)
    
    def run_conservative_augmentation_test(self, baseline_checkpoint: str = None, 
                                         quick_epochs: int = 3):
        """ë³´ìˆ˜ì  ì¦ê°• í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        ic("ë³´ìˆ˜ì  ì¦ê°• í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        if baseline_checkpoint:
            self.baseline_checkpoint = baseline_checkpoint
        
        # 1. ì ì§„ì  ì¦ê°• ì„¤ì • ìƒì„±
        progressive_configs = self.create_progressive_augmentation_configs()
        
        # 2. ê° ë‹¨ê³„ë³„ ë¹ ë¥¸ ê²€ì¦
        results = []
        for config in progressive_configs:
            try:
                result = self.run_quick_validation(config, epochs=quick_epochs)
                results.append(result)
                
                # ì¤‘ê°„ ê²°ê³¼ ì¶œë ¥
                ic(f"Phase: {config['experiment_name']}")
                ic(f"Val F1: {result.get('final_val_f1', 0):.4f}")
                
            except Exception as e:
                ic(f"ì˜¤ë¥˜ ë°œìƒ: {config['experiment_name']} - {e}")
                results.append({
                    'experiment_name': config['experiment_name'],
                    'error': str(e)
                })
        
        # 3. ì„±ëŠ¥ ë¹„êµ
        comparison = self.compare_with_baseline(results)
        
        # 4. ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = self.generate_recommendations(comparison)
        
        # 5. ê²°ê³¼ ì €ì¥
        summary_path = self.save_experiment_results(results, comparison, recommendations)
        
        ic("ë³´ìˆ˜ì  ì¦ê°• í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        ic(f"ê²°ê³¼ ë³´ê³ ì„œ: {summary_path}")
        
        return {
            'results': results,
            'comparison': comparison,
            'recommendations': recommendations,
            'summary_report': summary_path,
            'experiment_dir': str(self.experiment_dir)
        }


def main():
    """Fire CLI ì¸í„°í˜ì´ìŠ¤"""
    fire.Fire(ConservativeAugmentationTester)


if __name__ == "__main__":
    main()